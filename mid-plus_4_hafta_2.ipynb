{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2016e7",
   "metadata": {},
   "source": [
    "### Görev 1: Parametre Verimliliğini Kanıtlama (The \"Efficiency\" Challenge)\n",
    "\n",
    "Aşağıdaki kod bloğunda eksik bırakılan yerleri tamamlayın. Amacımız, LoRA uygulandığında eğitilebilir parametre sayısının ne kadar düştüğünü (veya ne kadar az parametre ile eğitim yapıldığını) matematiksel olarak görmektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2661b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Documents\\GitHub\\HuaweiStdDev-UludagUni-AIClub\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ASUS\\Documents\\GitHub\\HuaweiStdDev-UludagUni-AIClub\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baz model (GPT-2) — parametre durumu:\n",
      "Toplam parametre: 124439808\n",
      "Eğitilebilir parametre: 124439808 (100.00%)\n",
      "\n",
      "LoRA uygulanmış model — parametre durumu:\n",
      "Toplam parametre: 125250816\n",
      "Eğitilebilir parametre: 811008 (0.65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Documents\\GitHub\\HuaweiStdDev-UludagUni-AIClub\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Görev 1: Parametre Verimliliği — GPT-2 + LoRA\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Eğitimde açılacak parametre sayısını yazdır\n",
    "def print_trainable_parameters(m):\n",
    "    total = 0\n",
    "    trainable = 0\n",
    "    for p in m.parameters():\n",
    "        c = p.numel()\n",
    "        total += c\n",
    "        if p.requires_grad:\n",
    "            trainable += c\n",
    "    ratio = (trainable / total) * 100 if total > 0 else 0\n",
    "    print(f\"Toplam parametre: {total}\")\n",
    "    print(f\"Eğitilebilir parametre: {trainable} ({ratio:.2f}%)\")\n",
    "\n",
    "print(\"Baz model (GPT-2) — parametre durumu:\")\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# LoRA yapılandırması\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "print(\"\\nLoRA uygulanmış model — parametre durumu:\")\n",
    "print_trainable_parameters(peft_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76547e85",
   "metadata": {},
   "source": [
    "### Görev 2: LoRA Katmanlarını Gözlemleme (The \"Architecture\" Challenge)\n",
    "\n",
    "LoRA, modelin mevcut katmanlarına (genellikle Attention bloklarına) küçük matrisler ekler. Aşağıdaki kodu çalıştırarak `lora_A` ve `lora_B` katmanlarının modelin neresine eklendiğini çıktı üzerinde tespit edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be77e713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.transformer.h.0.attn.c_attn\n",
      "base_model.model.transformer.h.0.attn.c_proj\n",
      "base_model.model.transformer.h.0.mlp.c_proj\n",
      "base_model.model.transformer.h.1.attn.c_attn\n",
      "base_model.model.transformer.h.1.attn.c_proj\n",
      "base_model.model.transformer.h.1.mlp.c_proj\n",
      "base_model.model.transformer.h.2.attn.c_attn\n",
      "base_model.model.transformer.h.2.attn.c_proj\n",
      "base_model.model.transformer.h.2.mlp.c_proj\n",
      "base_model.model.transformer.h.3.attn.c_attn\n",
      "base_model.model.transformer.h.3.attn.c_proj\n",
      "base_model.model.transformer.h.3.mlp.c_proj\n",
      "base_model.model.transformer.h.4.attn.c_attn\n",
      "base_model.model.transformer.h.4.attn.c_proj\n",
      "base_model.model.transformer.h.4.mlp.c_proj\n",
      "base_model.model.transformer.h.5.attn.c_attn\n",
      "base_model.model.transformer.h.5.attn.c_proj\n",
      "base_model.model.transformer.h.5.mlp.c_proj\n",
      "base_model.model.transformer.h.6.attn.c_attn\n",
      "base_model.model.transformer.h.6.attn.c_proj\n",
      "base_model.model.transformer.h.6.mlp.c_proj\n",
      "base_model.model.transformer.h.7.attn.c_attn\n",
      "base_model.model.transformer.h.7.attn.c_proj\n",
      "base_model.model.transformer.h.7.mlp.c_proj\n",
      "base_model.model.transformer.h.8.attn.c_attn\n",
      "base_model.model.transformer.h.8.attn.c_proj\n",
      "base_model.model.transformer.h.8.mlp.c_proj\n",
      "base_model.model.transformer.h.9.attn.c_attn\n",
      "base_model.model.transformer.h.9.attn.c_proj\n",
      "base_model.model.transformer.h.9.mlp.c_proj\n",
      "base_model.model.transformer.h.10.attn.c_attn\n",
      "base_model.model.transformer.h.10.attn.c_proj\n",
      "base_model.model.transformer.h.10.mlp.c_proj\n",
      "base_model.model.transformer.h.11.attn.c_attn\n",
      "base_model.model.transformer.h.11.attn.c_proj\n",
      "base_model.model.transformer.h.11.mlp.c_proj\n",
      "\n",
      "Toplam LoRA adapte edilmiş alt modül sayısı: 36\n"
     ]
    }
   ],
   "source": [
    "# Görev 2: LoRA Katmanlarını Gözlemleme\n",
    "# Amaç: Modelde eklenen lora_A / lora_B katmanlarının yerlerini yazdırmak\n",
    "\n",
    "found = []\n",
    "for name, module in peft_model.named_modules():\n",
    "    has_a = hasattr(module, 'lora_A')\n",
    "    has_b = hasattr(module, 'lora_B')\n",
    "    if has_a or has_b:\n",
    "        found.append(name)\n",
    "        print(name)\n",
    "\n",
    "print(f\"\\nToplam LoRA adapte edilmiş alt modül sayısı: {len(found)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
