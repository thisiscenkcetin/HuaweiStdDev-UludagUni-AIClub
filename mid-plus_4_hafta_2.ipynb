{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2016e7",
   "metadata": {},
   "source": [
    "### Görev 1: Parametre Verimliliğini Kanıtlama (The \"Efficiency\" Challenge)\n",
    "\n",
    "Aşağıdaki kod bloğunda eksik bırakılan yerleri tamamlayın. Amacımız, LoRA uygulandığında eğitilebilir parametre sayısının ne kadar düştüğünü (veya ne kadar az parametre ile eğitim yapıldığını) matematiksel olarak görmektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661b607",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# 1. Base Modeli Yükle (Hafif olması için GPT-2 kullanıyoruz)\n",
    "model_name = \"gpt2\" \n",
    "print(f\"Model yükleniyor: {model_name}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Modeldeki eğitilebilir parametre sayısını ve tüm parametrelere oranını yazdırır.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    \n",
    "    # GÖREV: Modelin parametreleri üzerinde döngü kurarak sayım yapın\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            \n",
    "    print(f\"Eğitilebilir Parametre: {trainable_params} || Tümü: {all_param} || Oran: %{100 * trainable_params / all_param:.4f}\")\n",
    "\n",
    "print(\"\\n--- Base Model Durumu ---\")\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# 2. LoRA Konfigürasyonu Oluştur\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=8,            # Rank değeri\n",
    "    lora_alpha=32,  # Alpha ölçekleme değeri\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# 3. Modeli PEFT Model'e Dönüştür\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"\\n--- LoRA Entegre Edilmiş Model Durumu ---\")\n",
    "# Fonksiyonu tekrar çağırarak farkı görün\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76547e85",
   "metadata": {},
   "source": [
    "### Görev 2: LoRA Katmanlarını Gözlemleme (The \"Architecture\" Challenge)\n",
    "\n",
    "LoRA, modelin mevcut katmanlarına (genellikle Attention bloklarına) küçük matrisler ekler. Aşağıdaki kodu çalıştırarak `lora_A` ve `lora_B` katmanlarının modelin neresine eklendiğini çıktı üzerinde tespit edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77e713",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Modelin mimarisini yazdır\n",
    "print(\"Model Mimarisi ve LoRA Katmanları:\\n\")\n",
    "print(peft_model)\n",
    "\n",
    "# GÖREV İPUCU: Çıktı içerisinde \"lora_A\" ve \"lora_B\" ifadelerini arayın. \n",
    "# Hangi katmanların altında (Linear, Conv1D vb.) göründüğüne dikkat edin."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
