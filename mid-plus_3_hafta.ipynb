{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b5a011",
   "metadata": {},
   "source": [
    "**Değerlendirme ve Görevler**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac749e",
   "metadata": {},
   "source": [
    "[PyTorch RNN Tutorial - Name Classification Using A Recurrent Neural Net](https://www.youtube.com/watch?v=WEV61GmmPrk)\n",
    "\n",
    "*Videodaki projeyi yapınız*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fb781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Classification (Char-RNN) — Quick demo\n",
    "# Küçük bir karakter-seviyeli RNN ile isimleri (erkek/kadın) sınıflandırma örneği.\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Tiny synthetic dataset\n",
    "names = [\n",
    "    (\"Emma\", 0), (\"Olivia\", 0), (\"Ava\", 0), (\"Isabella\", 0), (\"Sophia\", 0),\n",
    "    (\"Liam\", 1), (\"Noah\", 1), (\"Oliver\", 1), (\"Elijah\", 1), (\"James\", 1)\n",
    "]\n",
    "\n",
    "# build char vocabulary\n",
    "chars = sorted(list({c.lower() for n,_ in names for c in n}))\n",
    "stoi = {c:i+1 for i,c in enumerate(chars)}  # 0 reserved for padding\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "vocab_size = len(stoi)+1\n",
    "\n",
    "max_len = max(len(n) for n,_ in names)\n",
    "\n",
    "def encode(name):\n",
    "    arr = [stoi[c.lower()] for c in name]\n",
    "    # pad\n",
    "    arr = arr + [0]*(max_len - len(arr))\n",
    "    return torch.tensor(arr, dtype=torch.long)\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.data = [(encode(n), torch.tensor(lbl, dtype=torch.long)) for n,lbl in pairs]\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.data[idx]\n",
    "\n",
    "ds = NameDataset(names)\n",
    "loader = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# Model: simple embedding + RNN (GRU) + classifier\n",
    "class SimpleNameRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb=16, hidden=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x)\n",
    "        out, h = self.gru(e)\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNameRNN(vocab_size).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# Quick training\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total=0; correct=0; running_loss=0.0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = crit(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = logits.argmax(dim=1)\n",
    "        total += yb.size(0); correct += (pred==yb).sum().item()\n",
    "    print(f\"Epoch {epoch+1}/5 — loss: {running_loss/len(loader):.4f} — acc: {correct/total:.2f}\")\n",
    "\n",
    "# Save\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "path = './models/name_rnn_quick.pth'\n",
    "torch.save({'model_state':model.state_dict(),'stoi':stoi,'max_len':max_len}, path)\n",
    "print('Saved model to', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad898308",
   "metadata": {},
   "source": [
    "[PyTorch Tutorial - RNN & LSTM & GRU - Recurrent Neural Nets](https://www.youtube.com/watch?v=0_PgWWmauHk)\n",
    "\n",
    "*Videodaki projeyi yapınız*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN vs LSTM vs GRU — tiny sequence modeling demo\n",
    "# Task: predict next value in a short numeric sequence (toy task) and show one training step per model.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# synthetic sequences: input seq of length 5, target = next value (sum of inputs mod 10)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.random.randint(0,10,(100,5)).astype(np.float32)/10.0\n",
    "Y = (X.sum(axis=1) % 1.0).astype(np.float32)  # small float target\n",
    "\n",
    "X = torch.tensor(X).unsqueeze(-1)  # (N,5,1)\n",
    "Y = torch.tensor(Y).unsqueeze(-1)  # (N,1)\n",
    "loader = DataLoader(TensorDataset(X,Y), batch_size=16, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SeqModel(nn.Module):\n",
    "    def __init__(self, rnn_type='RNN', input_size=1, hidden=16):\n",
    "        super().__init__()\n",
    "        if rnn_type=='RNN':\n",
    "            self.rnn = nn.RNN(input_size, hidden, batch_first=True)\n",
    "        elif rnn_type=='LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, h = self.rnn(x)\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "models = {t: SeqModel(t).to(device) for t in ('RNN','LSTM','GRU')}\n",
    "opt = {t: torch.optim.Adam(m.parameters(), lr=0.01) for t,m in models.items()}\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "# one epoch quick run\n",
    "for t,m in models.items():\n",
    "    m.train()\n",
    "    total_loss=0.0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        opt[t].zero_grad()\n",
    "        out = m(xb)\n",
    "        loss = crit(out, yb)\n",
    "        loss.backward()\n",
    "        opt[t].step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"{t} — avg loss: {total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488de6a",
   "metadata": {},
   "source": [
    "[Pytorch Transformers from Scratch (Attention is all you need)](https://www.youtube.com/watch?v=U0s0f995w14)\n",
    "\n",
    "*Videodaki projeyi yapınız*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf5230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers from Scratch — Multi-Head Self-Attention minimal implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([module for _ in range(N)])\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=32, num_heads=4):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, x):\n",
    "        B, T, E = x.shape\n",
    "        qkv = self.qkv(x)  # (B,T,3E)\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "        # reshape for heads\n",
    "        q = q.view(B, T, self.num_heads, self.d_k).transpose(1,2)  # (B,heads,T,d_k)\n",
    "        k = k.view(B, T, self.num_heads, self.d_k).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_heads, self.d_k).transpose(1,2)\n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)  # (B,heads,T,d_k)\n",
    "        out = out.transpose(1,2).contiguous().view(B,T,E)\n",
    "        return self.out(out)\n",
    "\n",
    "# Quick test\n",
    "x = torch.rand(2,10,32)\n",
    "attn = MultiHeadSelfAttention(32,4)\n",
    "out = attn(x)\n",
    "print('Input shape:', x.shape, 'Output shape:', out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e82926",
   "metadata": {},
   "source": [
    "[Implementing the Self-Attention Mechanism from Scratch in PyTorch!](https://www.youtube.com/watch?v=ZPLym9rJtM8)\n",
    "\n",
    "*Videodaki projeyi yapınız*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention (single-head) — compact implementation and check\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v, mask=None):\n",
    "    # q,k,v: (B, T, D)\n",
    "    scores = torch.matmul(q, k.transpose(-2,-1)) / (q.size(-1)**0.5)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(attn, v), attn\n",
    "\n",
    "B,T,D = 1,5,8\n",
    "q = torch.rand(B,T,D)\n",
    "k = torch.rand(B,T,D)\n",
    "v = torch.rand(B,T,D)\n",
    "out, attn = scaled_dot_product_attention(q,k,v)\n",
    "print('out.shape', out.shape, 'attn.shape', attn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd70f4",
   "metadata": {},
   "source": [
    "[Learn PyTorch in 5 Projects – Tutorial](https://www.youtube.com/watch?v=E0bwEAWmVEM&t=12974s)\n",
    "\n",
    "*Videodaki son projeyi yapınız (Transformer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef26e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal Transformer Encoder Block — tiny demo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=32, heads=4, ff_dim=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "# quick forward pass\n",
    "x = torch.rand(2,10,32)\n",
    "block = TransformerEncoderBlock(32, heads=4)\n",
    "out = block(x)\n",
    "print('Transformer block input', x.shape, 'output', out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
