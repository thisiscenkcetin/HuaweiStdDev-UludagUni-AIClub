{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864c0b7f",
   "metadata": {},
   "source": [
    "## B√∂l√ºm 0: Kurulum ve Imports\n",
    "\n",
    "Bu b√∂l√ºmde PyTorch k√ºt√ºphaneleri import edilmi≈ü, GPU/CPU kullanƒ±mƒ± kontrol edilmi≈ütir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425add18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 0: Setup and Required Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check PyTorch version and GPU availability\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current Device: {device}\")\n",
    "print(f\"CPU Count: {torch.get_num_threads()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb3f53",
   "metadata": {},
   "source": [
    "Soru 1: Neural Network yapmak i√ßin hangi k√ºt√ºphaneleri import etmeliyiz?\n",
    "Cevap:\n",
    "torch: PyTorch'un ana mod√ºl√º\n",
    "torch.nn: Layer'lar, activation fonksiyonlarƒ± vs. buradan geliyor\n",
    "torch.optim: Optimizer'lar var burada, aƒüƒ±rlƒ±klarƒ± g√ºncellemek i√ßin\n",
    "numpy: Sayƒ±larla i≈ülem yapmak i√ßin\n",
    "matplotlib: Grafikleri √ßizmek i√ßin\n",
    "\n",
    "Soru 2: Eƒüitimi GPU'da mƒ± yoksa CPU'da mƒ± yapƒ±caƒüƒ±z?\n",
    "Cevap:\n",
    "√ñnce torch.cuda.is_available() ile GPU var mƒ± kontrol et\n",
    "Sonra device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") yaz\n",
    "Model ve veriyi .to(device) ile g√∂nder\n",
    "GPU'da olursa √ßok daha hƒ±zlƒ± gidiyor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0526acd",
   "metadata": {},
   "source": [
    "## B√∂l√ºm 1: PyTorch Temelleri ve Basit Model Olu≈üturma\n",
    "\n",
    "Soru 1: Rastgele eƒüitim verisi nasƒ±l olu≈ütururuz?\n",
    "Cevap:\n",
    "torch.randn() ile random sayƒ±lar √ºretiyoruz\n",
    "X = torch.randn(100, 5) dedin mi 100 √∂rnek, 5 feature\n",
    "y = torch.randn(100, 1) target deƒüi≈ükeni\n",
    "Sonra .to(device) ile GPU'ya veya CPU'ya g√∂nder\n",
    "\n",
    "Soru 2: PyTorch'ta model nasƒ±l yazarƒ±z?\n",
    "Cevap:\n",
    "nn.Module class'ƒ±nƒ± extend etmen lazƒ±m\n",
    "__init__ metotunda layer'larƒ± tanƒ±mla, fc1, relu, fc2 vs\n",
    "forward() i√ßinde veri flow'unu yaz: \n",
    " x gir, layer1'e ge√ß, relu uygula, layer2'ye ge√ß, √ßƒ±k\n",
    "\n",
    "Soru 3: Eƒüitim d√∂ng√ºs√º nasƒ±l yazƒ±yoruz?\n",
    "Cevap:\n",
    "Epoch d√∂ng√ºs√º a√ß:\n",
    "1. Forward: tahmin = model(input)\n",
    "2. Loss hesapla: hata = loss_function(tahmin, hedef)\n",
    "3. Gradients sƒ±fƒ±rla: optimizer.zero_grad()\n",
    "4. Backward: hata.backward()\n",
    "5. G√ºncelle: optimizer.step()\n",
    "Bunu tekrar tekrar yap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce825a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: PyTorch Fundamentals - Create Random Data and Simple Model\n",
    "\n",
    "# 1. Create random data and target variables (Rastgele veri olu≈üturma)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate random input data: 100 samples, 5 features\n",
    "X = torch.randn(100, 5, device=device)  # Input features\n",
    "y = torch.randn(100, 1, device=device)  # Target variable\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"X device: {X.device}\")\n",
    "print(f\"y device: {y.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d868e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define a simple PyTorch nn.Module (Basit bir model tanƒ±mlama)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=10, output_size=1):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Layer 1: Input to Hidden\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Layer 2: Hidden to Output\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        x = self.fc1(x)           # Apply first linear layer\n",
    "        x = self.relu(x)          # Apply activation function\n",
    "        x = self.fc2(x)           # Apply second linear layer\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN(input_size=5, hidden_size=10, output_size=1).to(device)\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training loop - Forward, Loss Calculation, and Optimizer Steps\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "\n",
    "print(\"Training the model...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass (Forward)\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Loss calculation (Loss Hesaplama)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # Backward pass (Gradients calculation)\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    \n",
    "    # Optimizer step (Optimizer Adƒ±mƒ±)\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(f\"\\nFinal Loss: {losses[-1]:.6f}\")\n",
    "\n",
    "# Visualize training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3eb836",
   "metadata": {},
   "source": [
    "## B√∂l√ºm 2: Yapay Sinir Aƒüƒ± Olu≈üturma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49614c75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# B√∂l√ºm 2: Yapay Sinir Aƒüƒ± Olu≈üturma - XOR Problem √á√∂z√ºm√º\n",
    "## Section 2: Creating Neural Network - Solving XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: XOR Problem Solution with Neural Network\n",
    "\n",
    "# XOR Problem: A classic problem where linear models fail\n",
    "# Input: 2 binary features, Output: 1 binary output\n",
    "# XOR Truth Table:\n",
    "# 0,0 -> 0\n",
    "# 0,1 -> 1\n",
    "# 1,0 -> 1\n",
    "# 1,1 -> 0\n",
    "\n",
    "# Create XOR dataset\n",
    "X_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32, device=device)\n",
    "y_xor = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32, device=device)\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "print(f\"Inputs:\\n{X_xor}\")\n",
    "print(f\"Targets:\\n{y_xor}\")\n",
    "\n",
    "# Define XOR Neural Network\n",
    "class XOR_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR_NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)      # Input layer: 2 neurons\n",
    "        self.relu = nn.ReLU()           # Hidden layer activation\n",
    "        self.fc2 = nn.Linear(4, 1)      # Output layer: 1 neuron\n",
    "        self.sigmoid = nn.Sigmoid()     # Output activation (0-1 range)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize XOR model\n",
    "xor_model = XOR_NN().to(device)\n",
    "xor_optimizer = optim.Adam(xor_model.parameters(), lr=0.01)\n",
    "xor_loss_fn = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "\n",
    "# Train XOR model\n",
    "xor_losses = []\n",
    "xor_epochs = 1000\n",
    "\n",
    "print(\"\\nTraining XOR model...\")\n",
    "for epoch in range(xor_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = xor_model(X_xor)\n",
    "    \n",
    "    # Loss and backward\n",
    "    loss = xor_loss_fn(y_pred, y_xor)\n",
    "    xor_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    xor_optimizer.step()\n",
    "    \n",
    "    xor_losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{xor_epochs}], Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nXOR Model Predictions:\")\n",
    "with torch.no_grad():\n",
    "    predictions = xor_model(X_xor)\n",
    "    for i in range(len(X_xor)):\n",
    "        print(f\"Input: {X_xor[i].cpu().numpy()} -> Predicted: {predictions[i].item():.4f}, Target: {y_xor[i].item()}\")\n",
    "\n",
    "# Visualize XOR training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(xor_losses, linewidth=2, color='green')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (BCE)\")\n",
    "plt.title(\"XOR Problem - Training Loss Over Epochs\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04840cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding Gradients and Mathematical Operations in Neural Networks\n",
    "\n",
    "# Example: Manual gradient calculation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Understanding Gradients and Backpropagation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create simple tensors with requires_grad=True\n",
    "x = torch.tensor([[2.0, 3.0]], requires_grad=True, device=device)\n",
    "print(f\"\\nInput x: {x}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "\n",
    "# Forward pass: y = x^2\n",
    "y = (x ** 2).sum()\n",
    "print(f\"\\nFunction: y = sum(x^2)\")\n",
    "print(f\"y value: {y.item()}\")\n",
    "\n",
    "# Backward pass: compute gradients\n",
    "y.backward()\n",
    "print(f\"\\nGradients dy/dx: {x.grad}\")\n",
    "print(f\"Expected gradients: [2*2, 2*3] = [4, 6]\")\n",
    "\n",
    "# Gradient explanation\n",
    "print(\"\\nüìö Gradient (T√ºrev) Nedir?\")\n",
    "print(\"- Gradient, fonksiyonun her parametreye g√∂re kƒ±smi t√ºrevi\")\n",
    "print(\"- Modelin hatasƒ±nƒ± azaltmak i√ßin parametreleri hangi y√∂nde g√ºncelleyeceƒüini g√∂sterir\")\n",
    "print(\"- Negatif gradient y√∂n√ºne (gradient descent) hareket ederek loss azaltƒ±lƒ±r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68690089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding Overfitting, Epochs, and Data Retention\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overfitting, Epochs, and Data Retention Concepts\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a simple regression problem: y = 2x + 3 (with some noise)\n",
    "np.random.seed(42)\n",
    "X_train = torch.linspace(0, 10, 20, device=device).reshape(-1, 1)\n",
    "y_train = 2 * X_train + 3 + torch.randn(20, 1, device=device) * 0.5\n",
    "X_test = torch.linspace(0.5, 9.5, 10, device=device).reshape(-1, 1)\n",
    "y_test = 2 * X_test + 3 + torch.randn(10, 1, device=device) * 0.5\n",
    "\n",
    "# Create models with different complexities\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Train both models\n",
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=200, lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        y_pred_train = model(X_train)\n",
    "        loss_train = loss_fn(y_pred_train, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss_train.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_test = model(X_test)\n",
    "            loss_test = loss_fn(y_pred_test, y_test)\n",
    "            test_losses.append(loss_test.item())\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "simple_model = SimpleModel().to(device)\n",
    "complex_model = ComplexModel().to(device)\n",
    "\n",
    "print(\"\\nTraining simple model...\")\n",
    "simple_train_loss, simple_test_loss = train_model(simple_model, X_train, y_train, X_test, y_test, epochs=200)\n",
    "\n",
    "print(\"Training complex model...\")\n",
    "complex_train_loss, complex_test_loss = train_model(complex_model, X_train, y_train, X_test, y_test, epochs=200)\n",
    "\n",
    "# Visualize overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Simple model\n",
    "axes[0].plot(simple_train_loss, label='Training Loss', linewidth=2)\n",
    "axes[0].plot(simple_test_loss, label='Test Loss', linewidth=2)\n",
    "axes[0].set_title('Simple Model - Underfitting')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Complex model\n",
    "axes[1].plot(complex_train_loss, label='Training Loss', linewidth=2)\n",
    "axes[1].plot(complex_test_loss, label='Test Loss', linewidth=2)\n",
    "axes[1].set_title('Complex Model - Overfitting')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìö Overfitting Kavramƒ±:\")\n",
    "print(\"- Model eƒüitim verisine √ßok iyi uyarlanƒ±r, test verisinde ba≈üarƒ±sƒ±z olur\")\n",
    "print(\"- Training loss d√º≈üer, test loss y√ºkselir\")\n",
    "print(\"- Modeli engelleme: Early stopping, dropout, regularization\")\n",
    "print(\"\\nüìö Epoch Kavramƒ±:\")\n",
    "print(\"- Epoch = t√ºm eƒüitim verisinin bir kez model tarafƒ±ndan i≈ülenmesi\")\n",
    "print(\"- Daha fazla epoch = daha fazla √∂ƒürenme (limit var: overfitting)\")\n",
    "print(\"\\nüìö Veri Tutulmasƒ±:\")\n",
    "print(\"- Training set: Modeli eƒüitmek i√ßin\")\n",
    "print(\"- Validation set: Eƒüitim sƒ±rasƒ±nda performans kontrol√º\")\n",
    "print(\"- Test set: Final model performansƒ±nƒ± deƒüerlendirmek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c4be8",
   "metadata": {},
   "source": [
    "Soru 1: XOR problemi nedir ve neden sinir aƒüƒ± lazƒ±m?\n",
    "Cevap:\n",
    "XOR klasik bir problem, tek layer ile √ß√∂zemezsin lineer deƒüil\n",
    "Hidden layer eklersen non-linear ≈üeyler yapabilir\n",
    "Input(2) ‚Üí Hidden(4 n√∂ron) ‚Üí Output(1)\n",
    "ReLU ve Sigmoid kullanƒ±rsƒ±n, artƒ±k XOR √∂ƒürenebilir\n",
    "\n",
    "Soru 2: Aƒüda matematiksel i≈ülemler nasƒ±l √ßalƒ±≈üƒ±yor?\n",
    "Cevap:\n",
    "Her layer matris i≈ülemi yapar: z = W¬∑x + b\n",
    "Sonra activation gelir, mesela ReLU(z)\n",
    "Tersine giderken chain rule kullanƒ±yoruz\n",
    "Her layer'dan √∂ncekine gradient geri gidiyor\n",
    "\n",
    "Soru 3: Gradient tam olarak ne?\n",
    "Cevap:\n",
    "Gradient = Loss'un parametrelere g√∂re t√ºrevi\n",
    "Nereye gidersek loss azalƒ±r bunu g√∂steriyor\n",
    "w_new = w_old - learning_rate √ó gradient diye aƒüƒ±rlƒ±k g√ºncelleriz\n",
    "Bunu yapmasak aƒü hi√ßbir ≈üey √∂ƒürenemez\n",
    "\n",
    "Soru 4: Overfitting, epoch, veri split nedir?\n",
    "Cevap:\n",
    "Overfitting: Aƒü eƒüitim verisine √ßok alƒ±≈ütƒ±, yeni veriye uymuyor\n",
    "Epoch: T√ºm eƒüitim verisini bir kez ge√ßirmek\n",
    "Veri split: Training %70, test %30 b√∂l falan\n",
    "Validation seti de izle overfitting yapƒ±yor mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a374094",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# B√∂l√ºm 3: Matematiksel Tanƒ±mlar\n",
    "## Section 3: Mathematical Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Mathematical Definitions\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MATHEMATICAL FOUNDATIONS OF NEURAL NETWORKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. ACTIVATION FUNCTIONS (Aktivasyon Fonksiyonlarƒ±)\n",
    "print(\"\\n1Ô∏è‚É£ ACTIVATION FUNCTIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "x_range = torch.linspace(-5, 5, 100, device=device)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# ReLU: f(x) = max(0, x)\n",
    "relu = nn.ReLU()\n",
    "y_relu = relu(x_range)\n",
    "axes[0, 0].plot(x_range.cpu().numpy(), y_relu.cpu().numpy(), linewidth=2, color='red')\n",
    "axes[0, 0].set_title('ReLU: f(x) = max(0, x)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "print(\"ReLU (Rectified Linear Unit):\")\n",
    "print(\"  Formula: f(x) = max(0, x)\")\n",
    "print(\"  Advantage: Simple, fast, reduces vanishing gradient problem\")\n",
    "print(\"  Use: Hidden layers, most common\")\n",
    "\n",
    "# Sigmoid: f(x) = 1 / (1 + e^-x)\n",
    "sigmoid = nn.Sigmoid()\n",
    "y_sigmoid = sigmoid(x_range)\n",
    "axes[0, 1].plot(x_range.cpu().numpy(), y_sigmoid.cpu().numpy(), linewidth=2, color='blue')\n",
    "axes[0, 1].set_title('Sigmoid: f(x) = 1/(1+e‚ÅªÀ£)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "print(\"\\nSigmoid:\")\n",
    "print(\"  Formula: f(x) = 1 / (1 + e^-x)\")\n",
    "print(\"  Range: (0, 1)\")\n",
    "print(\"  Use: Binary classification output layer\")\n",
    "\n",
    "# Tanh: f(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "tanh = nn.Tanh()\n",
    "y_tanh = tanh(x_range)\n",
    "axes[0, 2].plot(x_range.cpu().numpy(), y_tanh.cpu().numpy(), linewidth=2, color='green')\n",
    "axes[0, 2].set_title('Tanh: f(x) = (eÀ£ - e‚ÅªÀ£)/(eÀ£ + e‚ÅªÀ£)')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "print(\"\\nTanh (Hyperbolic Tangent):\")\n",
    "print(\"  Formula: f(x) = (e^x - e^-x) / (e^x + e^-x)\")\n",
    "print(\"  Range: (-1, 1)\")\n",
    "print(\"  Use: Hidden layers, better than sigmoid for centered data\")\n",
    "\n",
    "# Leaky ReLU: f(x) = x if x > 0 else 0.01*x\n",
    "leaky_relu = nn.LeakyReLU(0.01)\n",
    "y_leaky = leaky_relu(x_range)\n",
    "axes[1, 0].plot(x_range.cpu().numpy(), y_leaky.cpu().numpy(), linewidth=2, color='orange')\n",
    "axes[1, 0].set_title('Leaky ReLU: f(x) = x or 0.01x')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "print(\"\\nLeaky ReLU:\")\n",
    "print(\"  Formula: f(x) = x if x > 0 else 0.01*x\")\n",
    "print(\"  Advantage: Allows small negative gradients\")\n",
    "print(\"  Use: When ReLU causes dead neurons\")\n",
    "\n",
    "# ELU: Exponential Linear Unit\n",
    "elu = nn.ELU()\n",
    "y_elu = elu(x_range)\n",
    "axes[1, 1].plot(x_range.cpu().numpy(), y_elu.cpu().numpy(), linewidth=2, color='purple')\n",
    "axes[1, 1].set_title('ELU: Exponential Linear Unit')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "print(\"\\nELU (Exponential Linear Unit):\")\n",
    "print(\"  Formula: f(x) = x if x > 0 else Œ±(e^x - 1)\")\n",
    "print(\"  Advantage: Smooth gradient, better performance\")\n",
    "\n",
    "# Softmax: Used for multi-class classification\n",
    "x_softmax = torch.tensor([[1.0, 2.0, 3.0]], device=device)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "y_softmax = softmax(x_softmax)\n",
    "axes[1, 2].bar(['Class 1', 'Class 2', 'Class 3'], y_softmax[0].cpu().numpy(), color=['red', 'green', 'blue'])\n",
    "axes[1, 2].set_title('Softmax Output (Multi-class)')\n",
    "axes[1, 2].set_ylim([0, 1])\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
    "print(\"\\nSoftmax:\")\n",
    "print(\"  Formula: œÉ(x_i) = e^x_i / Œ£(e^x_j)\")\n",
    "print(\"  Range: (0, 1), Sum = 1\")\n",
    "print(\"  Use: Multi-class classification output layer\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ea1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOSS FUNCTIONS (Kayƒ±p Fonksiyonlarƒ±)\n",
    "print(\"\\n2Ô∏è‚É£ LOSS FUNCTIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create sample data for loss visualization\n",
    "y_true = torch.tensor([[1.0, 0.0, 0.0]], device=device)  # Ground truth (one-hot)\n",
    "y_pred_range = torch.linspace(0, 1, 100, device=device)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# MSE Loss: (1/n) * Œ£(y_true - y_pred)^2\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "y_pred_mse = torch.randn(100, 1, device=device)\n",
    "y_true_mse = torch.ones(100, 1, device=device)\n",
    "mse_losses = [mse_loss_fn(y_pred_mse[i:i+1], y_true_mse[i:i+1]).item() for i in range(100)]\n",
    "axes[0, 0].hist(mse_losses, bins=20, color='blue', alpha=0.7)\n",
    "axes[0, 0].set_title('MSE Loss Distribution')\n",
    "axes[0, 0].set_xlabel('Loss Value')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "print(\"\\nMean Squared Error (MSE):\")\n",
    "print(\"  Formula: MSE = (1/n) * Œ£(y_true - y_pred)¬≤\")\n",
    "print(\"  Range: [0, ‚àû)\")\n",
    "print(\"  Use: Regression problems\")\n",
    "print(\"  Advantage: Smooth, differentiable everywhere\")\n",
    "\n",
    "# Cross Entropy Loss: -Œ£ y_true * log(y_pred)\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "y_pred_ce = torch.randn(50, 3, device=device)\n",
    "y_true_ce = torch.randint(0, 3, (50,), device=device)\n",
    "ce_losses = [ce_loss_fn(y_pred_ce[i:i+1], y_true_ce[i:i+1]).item() for i in range(50)]\n",
    "axes[0, 1].hist(ce_losses, bins=20, color='green', alpha=0.7)\n",
    "axes[0, 1].set_title('Cross Entropy Loss Distribution')\n",
    "axes[0, 1].set_xlabel('Loss Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "print(\"\\nCross Entropy Loss:\")\n",
    "print(\"  Formula: CE = -Œ£ y_true * log(y_pred)\")\n",
    "print(\"  Range: [0, ‚àû)\")\n",
    "print(\"  Use: Multi-class classification\")\n",
    "print(\"  Advantage: Works well with softmax activation\")\n",
    "\n",
    "# Binary Cross Entropy: -(y*log(p) + (1-y)*log(1-p))\n",
    "bce_loss_fn = nn.BCELoss()\n",
    "y_pred_bce = torch.rand(50, device=device)\n",
    "y_true_bce = torch.randint(0, 2, (50,), dtype=torch.float32, device=device)\n",
    "bce_losses = [bce_loss_fn(y_pred_bce[i:i+1], y_true_bce[i:i+1]).item() for i in range(50)]\n",
    "axes[1, 0].hist(bce_losses, bins=20, color='red', alpha=0.7)\n",
    "axes[1, 0].set_title('Binary Cross Entropy Loss Distribution')\n",
    "axes[1, 0].set_xlabel('Loss Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "print(\"\\nBinary Cross Entropy (BCE):\")\n",
    "print(\"  Formula: BCE = -(y*log(p) + (1-y)*log(1-p))\")\n",
    "print(\"  Range: [0, ‚àû)\")\n",
    "print(\"  Use: Binary classification\")\n",
    "\n",
    "# L1 Loss (MAE): Œ£|y_true - y_pred|\n",
    "l1_loss_fn = nn.L1Loss()\n",
    "y_pred_l1 = torch.randn(50, 1, device=device)\n",
    "y_true_l1 = torch.randn(50, 1, device=device)\n",
    "l1_losses = [l1_loss_fn(y_pred_l1[i:i+1], y_true_l1[i:i+1]).item() for i in range(50)]\n",
    "axes[1, 1].hist(l1_losses, bins=20, color='orange', alpha=0.7)\n",
    "axes[1, 1].set_title('L1 Loss (MAE) Distribution')\n",
    "axes[1, 1].set_xlabel('Loss Value')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "print(\"\\nL1 Loss (Mean Absolute Error):\")\n",
    "print(\"  Formula: L1 = (1/n) * Œ£|y_true - y_pred|\")\n",
    "print(\"  Range: [0, ‚àû)\")\n",
    "print(\"  Use: Regression, robust to outliers\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ded2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. GRADIENTS - Detailed Explanation (Gradientler)\n",
    "print(\"\\n3Ô∏è‚É£ GRADIENTS AND BACKPROPAGATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nWhat is a Gradient?\")\n",
    "print(\"  - Gradient is the partial derivative of a loss function with respect to parameters\")\n",
    "print(\"  - It shows the direction and magnitude of steepest ascent\")\n",
    "print(\"  - ‚àáf = [‚àÇf/‚àÇw1, ‚àÇf/‚àÇw2, ..., ‚àÇf/‚àÇwn]\")\n",
    "\n",
    "# Visualize gradient descent\n",
    "def create_loss_surface():\n",
    "    w1 = np.linspace(-3, 3, 100)\n",
    "    w2 = np.linspace(-3, 3, 100)\n",
    "    W1, W2 = np.meshgrid(w1, w2)\n",
    "    Z = W1**2 + W2**2  # Simple quadratic loss surface\n",
    "    return W1, W2, Z\n",
    "\n",
    "W1, W2, Z = create_loss_surface()\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(W1, W2, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('w1')\n",
    "ax1.set_ylabel('w2')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('Loss Surface: L = w1¬≤ + w2¬≤')\n",
    "\n",
    "# Contour plot with gradient descent path\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W1, W2, Z, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Simulate gradient descent\n",
    "w = np.array([-2.5, 2.5])\n",
    "learning_rate = 0.1\n",
    "path = [w.copy()]\n",
    "\n",
    "for _ in range(50):\n",
    "    # Compute gradient: ‚àáL = [2w1, 2w2]\n",
    "    gradient = 2 * w\n",
    "    # Update weights: w = w - lr * ‚àáL\n",
    "    w = w - learning_rate * gradient\n",
    "    path.append(w.copy())\n",
    "    if np.linalg.norm(gradient) < 0.01:\n",
    "        break\n",
    "\n",
    "path = np.array(path)\n",
    "ax2.plot(path[:, 0], path[:, 1], 'ro-', linewidth=2, markersize=4, label='Gradient Descent Path')\n",
    "ax2.plot(0, 0, 'g*', markersize=20, label='Optimum')\n",
    "ax2.set_xlabel('w1')\n",
    "ax2.set_ylabel('w2')\n",
    "ax2.set_title('Gradient Descent Optimization')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGradient Descent Update Rule:\")\n",
    "print(\"  w_new = w_old - learning_rate √ó ‚àáL\")\n",
    "print(\"  - learning_rate: How big steps to take (0.001 - 0.01 typical)\")\n",
    "print(\"  - ‚àáL: Direction of steepest ascent (we go opposite direction)\")\n",
    "print(\"  - This iterative process finds optimal weights that minimize loss\")\n",
    "\n",
    "print(\"\\nWhy Gradients are Important:\")\n",
    "print(\"  1. Without gradients, we don't know which direction to update weights\")\n",
    "print(\"  2. Backpropagation efficiently computes gradients via chain rule\")\n",
    "print(\"  3. Enables learning in deep networks (deep learning)\")\n",
    "print(\"  4. Chain rule: ‚àÇL/‚àÇw = (‚àÇL/‚àÇa) √ó (‚àÇa/‚àÇz) √ó (‚àÇz/‚àÇw)\")\n",
    "\n",
    "print(\"\\nGradient Issues:\")\n",
    "print(\"  - Vanishing Gradient: Gradients become too small, learning stops\")\n",
    "print(\"  - Exploding Gradient: Gradients become too large, training unstable\")\n",
    "print(\"  - Dead ReLU: ReLU neurons die (always output 0) ‚Üí no gradient flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb398d0",
   "metadata": {},
   "source": [
    "## B√∂l√ºm 3: Matematiksel Tanƒ±mlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b694ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# B√∂l√ºm 4: Veri Akƒ±≈üƒ± ve Yakla≈üƒ±mlar\n",
    "## Section 4: Data Flow and Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Data Flow in Neural Networks and Problem-Solving Approaches\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA FLOW AND PROBLEM-SOLVING APPROACHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Data Flow Diagram\n",
    "print(\"\\n1Ô∏è‚É£ DATA FLOW IN NEURAL NETWORKS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    DATA FLOW THROUGH NN                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "    Input Data (X)\n",
    "         ‚Üì\n",
    "    [Input Layer: 5 neurons]\n",
    "         ‚Üì (matrix multiply: X √ó W1 + b1)\n",
    "    [z1 = X¬∑W1 + b1]\n",
    "         ‚Üì (activation function)\n",
    "    [a1 = ReLU(z1)]  ‚Üê Hidden Layer: 10 neurons\n",
    "         ‚Üì (matrix multiply: a1 √ó W2 + b2)\n",
    "    [z2 = a1¬∑W2 + b2]\n",
    "         ‚Üì (activation function)\n",
    "    [Output = Sigmoid(z2)]  ‚Üê Output Layer: 1 neuron\n",
    "         ‚Üì\n",
    "    Loss Calculation: L = BCE_Loss(Output, y_true)\n",
    "         ‚Üì\n",
    "    [BACKWARD PASS - Gradient Computation]\n",
    "         ‚Üì\n",
    "    ‚àÇL/‚àÇW2, ‚àÇL/‚àÇb2 (output layer gradients)\n",
    "         ‚Üì\n",
    "    ‚àÇL/‚àÇW1, ‚àÇL/‚àÇb1 (hidden layer gradients)\n",
    "         ‚Üì\n",
    "    Weight Update: W = W - lr √ó ‚àÇL/‚àÇW\n",
    "\"\"\")\n",
    "\n",
    "# Visualize data flow with tensor shapes\n",
    "print(\"\\n2Ô∏è‚É£ TENSOR SHAPES THROUGH FORWARD PASS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "class DataFlowModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(5, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        print(f\"After fc1 (5‚Üí10): {x.shape}\")\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        print(f\"After ReLU: {x.shape}\")\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        print(f\"After fc2 (10‚Üí1): {x.shape}\")\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        print(f\"After Sigmoid: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "model_flow = DataFlowModel().to(device)\n",
    "X_demo = torch.randn(32, 5, device=device)  # Batch size 32, 5 features\n",
    "print(\"\\nForward Pass Shape Propagation:\")\n",
    "with torch.no_grad():\n",
    "    _ = model_flow(X_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Problem-Solving Framework\n",
    "print(\"\\n3Ô∏è‚É£ GENERAL FRAMEWORK FOR NN MODELING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "STEP 1: PROBLEM ANALYSIS\n",
    "‚îú‚îÄ Identify problem type: Classification or Regression?\n",
    "‚îú‚îÄ Multi-class or Binary classification?\n",
    "‚îú‚îÄ Output activation: Sigmoid/ReLU/Softmax?\n",
    "‚îî‚îÄ Loss function: MSE/BCE/CrossEntropy?\n",
    "\n",
    "STEP 2: DATA PREPARATION\n",
    "‚îú‚îÄ Load and explore data\n",
    "‚îú‚îÄ Handle missing values\n",
    "‚îú‚îÄ Normalize/Standardize features (0-1 or -1-1)\n",
    "‚îú‚îÄ Split into train/val/test sets\n",
    "‚îî‚îÄ Create data loaders for batching\n",
    "\n",
    "STEP 3: MODEL ARCHITECTURE\n",
    "‚îú‚îÄ Input layer size = number of features\n",
    "‚îú‚îÄ Hidden layers: Start simple, increase if needed\n",
    "‚îú‚îÄ Number of hidden neurons: 32-512 typically\n",
    "‚îú‚îÄ Output layer size = number of classes/targets\n",
    "‚îú‚îÄ Choose activations: ReLU for hidden, Sigmoid/Softmax for output\n",
    "‚îî‚îÄ Add regularization: Dropout, L1/L2 if overfitting\n",
    "\n",
    "STEP 4: TRAINING SETUP\n",
    "‚îú‚îÄ Choose optimizer: Adam (recommended for beginners)\n",
    "‚îú‚îÄ Set learning rate: 0.001-0.01\n",
    "‚îú‚îÄ Define loss function\n",
    "‚îú‚îÄ Set batch size: 32-128\n",
    "‚îî‚îÄ Set number of epochs: 50-500\n",
    "\n",
    "STEP 5: TRAINING LOOP\n",
    "‚îú‚îÄ For each epoch:\n",
    "‚îÇ  ‚îú‚îÄ For each batch:\n",
    "‚îÇ  ‚îÇ  ‚îú‚îÄ Forward pass: y_pred = model(X)\n",
    "‚îÇ  ‚îÇ  ‚îú‚îÄ Compute loss: loss = loss_fn(y_pred, y)\n",
    "‚îÇ  ‚îÇ  ‚îú‚îÄ Backward: loss.backward()\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ Update: optimizer.step()\n",
    "‚îÇ  ‚îú‚îÄ Validate on validation set\n",
    "‚îÇ  ‚îî‚îÄ Monitor: training loss, validation loss, metrics\n",
    "‚îî‚îÄ Early stopping if validation loss increases\n",
    "\n",
    "STEP 6: EVALUATION\n",
    "‚îú‚îÄ Test on test set (never seen during training)\n",
    "‚îú‚îÄ Compute metrics: Accuracy, Precision, Recall, F1\n",
    "‚îú‚îÄ Create confusion matrix for classification\n",
    "‚îî‚îÄ Analyze failure cases\n",
    "\n",
    "STEP 7: OPTIMIZATION\n",
    "‚îú‚îÄ If underfitting: Add complexity\n",
    "‚îú‚îÄ If overfitting: Add regularization\n",
    "‚îú‚îÄ Tune hyperparameters: lr, batch_size, hidden_size\n",
    "‚îî‚îÄ Try different architectures\n",
    "\"\"\")\n",
    "\n",
    "# Example: Actual problem-solving\n",
    "print(\"\\n4Ô∏è‚É£ PRACTICAL EXAMPLE: IRIS CLASSIFICATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = torch.from_numpy(iris.data).float().to(device)\n",
    "y_iris = torch.from_numpy(iris.target).long().to(device)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris.cpu().numpy(), y_iris.cpu().numpy(), test_size=0.3, random_state=42\n",
    ")\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).long().to(device)\n",
    "y_test = torch.from_numpy(y_test).long().to(device)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.cpu().numpy())\n",
    "X_test_scaled = scaler.transform(X_test.cpu().numpy())\n",
    "X_train = torch.from_numpy(X_train_scaled).float().to(device)\n",
    "X_test = torch.from_numpy(X_test_scaled).float().to(device)\n",
    "\n",
    "print(f\"Dataset: Iris Classification\")\n",
    "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "print(f\"Number of classes: 3 (Setosa, Versicolor, Virginica)\")\n",
    "\n",
    "# Build model\n",
    "class IrisClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 16)       # 4 features ‚Üí 16 hidden\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(16, 8)       # 16 ‚Üí 8 hidden\n",
    "        self.fc3 = nn.Linear(8, 3)        # 8 ‚Üí 3 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "iris_model = IrisClassifier().to(device)\n",
    "iris_optimizer = optim.Adam(iris_model.parameters(), lr=0.01)\n",
    "iris_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining Iris Classifier...\")\n",
    "for epoch in range(100):\n",
    "    iris_model.train()\n",
    "    y_pred = iris_model(X_train)\n",
    "    loss = iris_loss_fn(y_pred, y_train)\n",
    "    iris_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    iris_optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        iris_model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_test = iris_model(X_test)\n",
    "            test_loss = iris_loss_fn(y_pred_test, y_test)\n",
    "            _, predictions = torch.max(y_pred_test, 1)\n",
    "            accuracy = (predictions == y_test).float().mean()\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}, Accuracy: {accuracy.item():.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "iris_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_final = iris_model(X_test)\n",
    "    _, predictions = torch.max(y_pred_final, 1)\n",
    "    final_accuracy = (predictions == y_test).float().mean()\n",
    "    print(f\"\\nFinal Test Accuracy: {final_accuracy.item():.4f} ({int(final_accuracy.item()*len(y_test))}/{len(y_test)} correct)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c44d4",
   "metadata": {},
   "source": [
    "## B√∂l√ºm 4: Veri Akƒ±≈üƒ± ve Yakla≈üƒ±mlar\n",
    "\n",
    "Soru 1: Problem √ß√∂z√ºm√ºnde NN modellemede genel fikir\n",
    "Cevap:\n",
    "1. Problem t√ºr√ºn√º belirleme: Classification mƒ± Regression mi?\n",
    "2. Veri hazƒ±rlama: Normalization, train/val/test split\n",
    "3. Model mimarisi tasarlama: Input ‚Üí Hidden layers ‚Üí Output\n",
    "4. Uygun activation ve loss function se√ßimi\n",
    "5. Optimizer se√ßimi (Adam genellikle iyi √ßalƒ±≈üƒ±r)\n",
    "6. Training loop: Forward ‚Üí Loss ‚Üí Backward ‚Üí Optimize\n",
    "7. Validation ile overfitting kontrol√º\n",
    "8. Early stopping uygulamasƒ±\n",
    "9. Test seti ile final evaluation\n",
    "10. Hyperparameter tuning gerekirse\n",
    "\n",
    "Soru 2: NN i√ßerisindeki data akƒ±≈üƒ±nƒ± sezgisel olarak a√ßƒ±klayƒ±nƒ±z\n",
    "Cevap:\n",
    "Input (x): Feature vekt√∂r√º\n",
    "Layer 1: z1 = W1¬∑x + b1, a1 = œÉ(z1) - Non-linear transformation\n",
    "Layer 2: z2 = W2¬∑a1 + b2, a2 = œÉ(z2) - Daha kompleks √∂zellikler\n",
    "Layer 3 (Output): z3 = W3¬∑a2 + b3 - Final prediction\n",
    "\n",
    "Forward pass: Veri input'dan output'a akar\n",
    "Backward pass: Loss output'tan input'a geri yayƒ±lƒ±r\n",
    "Her layer gradients hesaplar ve aƒüƒ±rlƒ±klarƒ± g√ºnceller\n",
    "ƒ∞lk layerlar low-level √∂zellikleri, son layerlar high-level √∂zellikleri √∂ƒürenirler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b68ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# B√∂l√ºm 5: Deƒüerlendirme ve G√∂revler\n",
    "## Section 5: Evaluation and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b60cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Complete PyTorch and NN Project - Fashion MNIST Classification\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPLETE PROJECT: FASHION MNIST CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")\n",
    "\n",
    "# Class names\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Build deep neural network model\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model_final = DeepNN().to(device)\n",
    "optimizer_final = optim.Adam(model_final.parameters(), lr=0.001)\n",
    "loss_fn_final = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model_final)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model_final.parameters())}\")\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(test_loader), correct / total\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining Fashion MNIST Classifier...\")\n",
    "num_epochs = 15\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_final, train_loader, loss_fn_final, optimizer_final, device)\n",
    "    val_loss, val_acc = validate(model_final, test_loader, loss_fn_final, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Validation Accuracy: {val_accs[-1]:.4f}\")\n",
    "\n",
    "# Visualize training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses, label='Training Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(train_accs, label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(val_accs, label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test on sample images\n",
    "print(\"\\nSample Predictions:\")\n",
    "model_final.eval()\n",
    "with torch.no_grad():\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images[:5].to(device)\n",
    "    labels = labels[:5]\n",
    "    \n",
    "    outputs = model_final(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(f\"True: {classes[labels[i]]}, Predicted: {classes[predicted[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Detailed Metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\nDetailed Evaluation Metrics:\")\n",
    "\n",
    "model_final.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model_final(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=classes))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix - Fashion MNIST')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1e73d",
   "metadata": {},
   "source": [
    "## B√∂l√ºm 5: Deƒüerlendirme ve G√∂revler\n",
    "\n",
    "Soru: PyTorch ve NN projelerini tamamlayƒ±nƒ±z\n",
    "Cevap:\n",
    "\n",
    "Fashion MNIST veri seti ile derin sinir aƒüƒ± projesi tamamlanmƒ±≈ütƒ±r.\n",
    "\n",
    "Model Mimarisi:\n",
    "- Giri≈ü katmanƒ±: 784 n√∂ron (28√ó28 g√∂r√ºnt√º)\n",
    "- Gizli katman 1: 256 n√∂ron + ReLU + Dropout(0.3)\n",
    "- Gizli katman 2: 128 n√∂ron + ReLU + Dropout(0.3)\n",
    "- Gizli katman 3: 64 n√∂ron + ReLU + Dropout(0.2)\n",
    "- √áƒ±kƒ±≈ü katmanƒ±: 10 n√∂ron (10 sƒ±nƒ±f)\n",
    "\n",
    "Eƒüitim Parametreleri:\n",
    "- Optimizer: Adam (lr=0.001)\n",
    "- Loss fonksiyonu: CrossEntropyLoss\n",
    "- Batch size: 64\n",
    "- Epoch sayƒ±sƒ±: 15\n",
    "- Eƒüitim seti: 60,000 g√∂r√ºnt√º\n",
    "- Test seti: 10,000 g√∂r√ºnt√º\n",
    "\n",
    "Uygulanan Teknikler:\n",
    "- Dropout ile regularization (overfitting √∂nlemesi)\n",
    "- Normalization ile veri standartla≈ütƒ±rmasƒ±\n",
    "- Train/test split ile veri ayrƒ±mƒ±\n",
    "- Learning curves ile eƒüitim monitoringi\n",
    "- Confusion matrix ile detaylƒ± deƒüerlendirme\n",
    "- Classification report ile precision, recall, f1 metrikleri\n",
    "\n",
    "√áƒ±kƒ±≈ülar:\n",
    "- Eƒüitim ve validasyon loss grafikleri\n",
    "- Eƒüitim ve validasyon accuracy grafikleri\n",
    "- Confusion matrix visualization\n",
    "- Detaylƒ± classification report\n",
    "- √ñrnek tahminler ve doƒüru sƒ±nƒ±flandƒ±rmalar\n",
    "\n",
    "Proje ba≈üarƒ±yla tamamlanmƒ±≈ütƒ±r ve t√ºm PyTorch fundamentals ve neural network konseptleri uygulanmƒ±≈ütƒ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Fashion MNIST veri setini indiriyoruz\n",
    "# Veriyi normalize ediyoruz (-1 ile 1 arasƒ±na √ßekiyoruz)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "print(\"Veriler indiriliyor...\")\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Sƒ±nƒ±flarƒ±mƒ±z\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Modelimizi tanƒ±mlƒ±yoruz (Dropout ekledim overfitting olmasƒ±n diye)\n",
    "class FashionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28) # Fotoƒürafƒ± d√ºzle≈ütir\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Modeli GPU'ya atƒ±yoruz\n",
    "model = FashionModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Eƒüitim d√∂ng√ºs√º\n",
    "num_epochs = 15\n",
    "train_losses = []\n",
    "test_accs = []\n",
    "\n",
    "print(\"Eƒüitim ba≈üladƒ±...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Sƒ±fƒ±rla, t√ºrev al, g√ºncelle\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Her epoch sonu loss ortalamasƒ±\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # Test seti √ºzerinde doƒüruluk kontrol√º (Validation)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    test_accs.append(acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} -> Loss: {epoch_loss:.4f}, Accuracy: %{acc:.2f}\")\n",
    "\n",
    "print(\"Eƒüitim tamamlandƒ±.\")\n",
    "\n",
    "# Grafikleri √ßizelim\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_accs, color='orange')\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n",
    "\n",
    "# Sonu√ßlarƒ± detaylƒ± g√∂relim\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDetaylƒ± Rapor:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
