{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1e6c30",
   "metadata": {},
   "source": [
    "# Yapay Zeka Topluluğu | Yapay Zeka Gelişim Kampı \n",
    "###### Middle Plus Hafta 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf05107",
   "metadata": {},
   "source": [
    "#### İçindekiler\n",
    "\n",
    "**Recurrent Neural Networks (RNN)**\n",
    "\n",
    "- *Zaman serileri ve metin için*\n",
    "\n",
    "- *LSTM, GRU*\n",
    "\n",
    "- *Uygulamalar: Metin tahmini, dil modelleme*\n",
    "\n",
    "**Transformer ve Attention**\n",
    "\n",
    "- *NLP’de modern standart*\n",
    "\n",
    "- *BERT, GPT modelleri*\n",
    "\n",
    "- *Self-Attention, Positional Encoding*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e303b52",
   "metadata": {},
   "source": [
    "| Ağ Türü              | Veri Tipi           | Avantajı                       | Tipik Uygulama                      |\n",
    "| -------------------- | ------------------- | ------------------------------ | ----------------------------------- |\n",
    "| **CNN**              | Görüntü             | Uzamsal paternleri yakalar     | Görüntü tanıma, otonom araç         |\n",
    "| **RNN / LSTM / GRU** | Zaman serisi, metin | Zaman bağımlılığı öğrenir      | Metin tahmini, dil modelleme        |\n",
    "| **Transformer**      | Metin, sekans       | Paralel işlem ve bağlam anlama | ChatGPT, BERT                       |\n",
    "| **VAE / GAN**        | Görsel / Ses        | Veri üretimi ve sentez         | Görüntü üretimi, sanat, ses sentezi |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ebfd5",
   "metadata": {},
   "source": [
    "**Büyük Dil Modelleri (Large Language Models - LLM): Teknik Derinlik, Mimari ve Tarihsel Evrim**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe994b",
   "metadata": {},
   "source": [
    "Büyük Dil Modelleri, milyarlarca parametre içeren ve devasa metin verileri üzerinde eğitilerek dilin istatistiksel yapısını öğrenen yapay sinir ağlarıdır. Temel amaçları basittir: Bir sonraki token'ı (kelime parçasını) tahmin etmek\n",
    "\n",
    "Matematiksel olarak bir LLM, bir kelime dizisi $w_1, w_2, ..., w_t$ verildiğinde, bir sonraki $w_{t+1}$ kelimesinin koşullu olasılığını maksimize etmeye çalışır:\n",
    "\n",
    "$$P(w) = \\prod_{t=1}^{T} P(w_t | w_1, ..., w_{t-1})$$\n",
    "\n",
    "Burada model, önceki tüm kelimelerin bağlamına (context) bakarak sıradaki kelime için bir olasılık dağılımı oluşturur.\n",
    "\n",
    "LLM'lerin bugünkü haline gelmesi, on yıllar süren bir evrimin sonucudur:\n",
    "\n",
    "- **N-Gram Modelleri:** İstatistiksel olarak kelimelerin yan yana gelme sıklığını sayan basit modeller. Bağlamı hatırlama yetenekleri çok kısıtlıydı.\n",
    "\n",
    "- **RNN (Recurrent Neural Networks):** Kelimeleri sırayla işleyen sinir ağları. Ancak \"Vanishing Gradient\" (Kaybolan Gradyan) problemi nedeniyle uzun cümlelerin başını unutuyorlardı.\n",
    "\n",
    "- **LSTM (Long Short-Term Memory):** RNN'in hafıza sorununu çözen, kapı (gate) mekanizmalı yapılar. 2017 öncesi Google Translate'in arkasındaki güçtü.\n",
    "\n",
    "- **Transformer (2017 - Kırılma Noktası):** Google araştırmacılarının \"Attention Is All You Need\" makalesiyle duyurduğu, veriyi sırayla değil paralel işleyen devrimsel mimari.\n",
    "\n",
    "Modern LLM'lerin (GPT, Claude, Llama, Gemini) tamamı Transformer mimarisine dayanır. Bu mimariyi anlamak, LLM'lerin nasıl \"düşündüğünü\" anlamaktır.\n",
    "\n",
    "**Tokenization (Metni Sayılara Çevirmek)**\n",
    "Bilgisayarlar kelimeleri anlamaz, sayıları anlar. Bu yüzden metin önce \"token\"lara bölünür.\n",
    "\n",
    "**Kelime bazlı:** Her kelime bir token (Örn: \"elma\"). Kelime hazinesi çok büyür.\n",
    "\n",
    "**Karakter bazlı:** Her harf bir token. Anlamı yakalamak zorlaşır.\n",
    "\n",
    "**Sub-word (Alt kelime) Tokenization:** Günümüzde kullanılan yöntem (Örn: BPE - Byte Pair Encoding). \"Kalemlik\" kelimesi \"Kalem\" + \"lik\" olarak bölünebilir.\n",
    "\n",
    "**Not:** Ortalama 1000 token, İngilizce'de yaklaşık 750 kelimeye, Türkçe'de ise eklemeli dil yapısı nedeniyle biraz daha az kelimeye denk gelir.\n",
    "\n",
    " **Embeddings (Vektör Uzayı)**\n",
    " \n",
    " Token'lar sayılara çevrildikten sonra, çok boyutlu bir vektör uzayına (Örn: 12.288 boyutlu) yerleştirilir. Bu uzayda anlamca birbirine yakın kelimeler, matematiksel olarak da birbirine yakındır.\n",
    "\n",
    " **Örnek:**$\\text{Kral} - \\text{Erkek} + \\text{Kadın} \\approx \\text{Kraliçe}$Bu işlem, modelin kelimeler arasındaki anlamsal ilişkiyi (cinsiyet, makam vb.) vektörler üzerinden öğrendiğini gösterir.\n",
    "\n",
    " **Self-Attention (Öz Dikkat) Mekanizması**\n",
    "\n",
    " Transformer'ın en önemli parçasıdır. Modelin bir kelimeyi işlerken, cümledeki diğer tüm kelimelere ne kadar odaklanması gerektiğini hesaplar.\n",
    "\n",
    " Üç ana bileşen vardır (Veritabanı analojisi ile):\n",
    "\n",
    " - Query (Q - Sorgu): \"Ben ne arıyorum?\" (Örn: 'O' zamiri kimi kastediyor?)\n",
    "\n",
    " - Key (K - Anahtar): \"Ben neyim?\" (Örn: Ben 'Ahmet' ismiyim.)\n",
    " \n",
    " - Value (V - Değer): \"İçeriğim ne?\" (Örn: Ahmet'in vektör temsili.\n",
    "\n",
    " Attention Formülü:\n",
    " $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    " \n",
    " Açıklama:\n",
    "\n",
    " - $QK^T$: Sorgu ve Anahtarların çarpımı (Benzerlik hesaplar).\n",
    "\n",
    " - $\\sqrt{d_k}$: Stabilizasyon için ölçekleme.\n",
    "\n",
    " - softmax: Skorları 0 ile 1 arasına sıkıştırıp olasılığa çevirir.\n",
    "\n",
    " - $V$: Sonuç, değer vektörleri ile ağırlıklandırılır.\n",
    "\n",
    " Bu sayede model, \"Ahmet topu attı ve o camı kırdı\" cümlesindeki \"o\" zamirinin \"top\" değil \"Ahmet\" ile ilişkili olduğunu, aradaki mesafeye bakmaksızın anlar.\n",
    "\n",
    " **Positional Encoding (Konum Kodlaması)**\n",
    "\n",
    " Transformer veriyi paralel işlediği için kelimelerin sırasını bilmez. \"Ali Ayşe'yi seviyor\" ile \"Ayşe Ali'yi seviyor\" aynı kelimelere sahiptir ama anlam farklıdır. Bu yüzden her token'ın vektörüne, cümledeki sırasını belirten özel bir sinüs/kosinüs dalgası (Positional Encoding) eklenir.\n",
    "\n",
    " **Feed-Forward Networks & Layer Norm**\n",
    "\n",
    "Attention katmanından çıkan bilgi, derin sinir ağlarından (MLP) geçirilir ve Layer Normalization ile veriler stabilize edilir. Bu, modelin çok derinleşmesine (100+ katman) rağmen kararlı bir şekilde eğitilmesini sağlar.\n",
    "\n",
    "**Mimari Türleri**\n",
    "\n",
    "- **Encoder-Only (Sadece Kodlayıcı):** Metni anlamak ve sınıflandırmak için iyidir. (Örn: BERT). Metin üretmez, metni analiz eder.\n",
    "\n",
    "- **Decoder-Only (Sadece Kod Çözücü):** Metin üretimi için tasarlanmıştır. (Örn: GPT serisi, Llama). Verilen girdiden sonrasını tahmin eder. Günümüzdeki popüler LLM'lerin çoğu budur.\n",
    "\n",
    "- **Encoder-Decoder:** Hem anlama hem üretme (çeviri gibi) işlerinde kullanılır. (Örn: T5, Google Translate).\n",
    "\n",
    "**Parametre Sayısı Nedir?**\n",
    "\n",
    "Parametreler (Weights & Biases), modelin eğitim sırasında öğrendiği ve sabitlediği sayısal değerlerdir.\n",
    "\n",
    "- **7B (7 Milyar):** Kişisel bilgisayarlarda çalışabilir, hızlıdır ama mantıksal çıkarım yeteneği sınırlıdır.\n",
    "\n",
    "- **70B:** Güçlü GPU'lar gerektirir, çok daha akıllıdır.\n",
    "\n",
    "- **1T+ (1 Trilyon üstü):** GPT-4 gibi devasa modeller. \"Emergent Properties\" (Beliren Özellikler) bu boyutta ortaya çıkar (Örn: Kod yazma, mizahı anlama, çok adımlı mantık yürütme).\n",
    "\n",
    "| Model Ailesi | Geliştirici | Tür           | Özellikler                                                                          |\n",
    "|--------------|-------------|---------------|--------------------------------------------------------------------------------------|\n",
    "| GPT-4        | OpenAI      | Kapalı Kaynak | Çok yüksek mantık yeteneği, multimodal (görüntü/metin).                             |\n",
    "| Llama 3      | Meta        | Açık Ağırlık  | Araştırma ve ticari kullanım için güçlü açık model.                                 |\n",
    "| Claude 3.5   | Anthropic   | Kapalı Kaynak | Uzun bağlam penceresi (context window) ve güvenli kodlama.                          |\n",
    "| Mistral      | Mistral AI  | Açık Ağırlık  | Düşük parametre sayısıyla (7B) yüksek performans (verimlilik odaklı).               |\n",
    "\n",
    "**Bağlam Penceresi (Context Window)\n",
    "\n",
    "Bağlam penceresi, modelin \"kısa süreli hafızası\"dır. Bir konuşma sırasında modelin geriye dönük ne kadar metni hatırlayabileceğini belirler.\n",
    "\n",
    "- **Standart:** 4K - 8K token (Yaklaşık 10-20 sayfa).\n",
    "\n",
    "- **Geniş:** 128K token (Yaklaşık 300 sayfa kitap).\n",
    "\n",
    "- **Ultra Geniş:** 1M+ token (Gemini 1.5 Pro gibi modeller milyonlarca token işleyebilir).\n",
    "\n",
    "**Teknik Zorluk:** Bağlam penceresi iki katına çıktığında, işlem maliyeti (Attention mekanizmasındaki matris çarpımı nedeniyle) karesel olarak ($O(n^2)$) artar. Bu yüzden \"Flash Attention\" gibi optimizasyon teknikleri geliştirilmiştir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146b0b7",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec49ed7",
   "metadata": {},
   "source": [
    "**Recurrent Neural Network (RNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb27f6c2",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNN) veya Türkçesiyle Yinelemeli Sinir Ağları, yapay zekanın özellikle \"sıralı verileri\" (sequential data) işlemek için tasarlanmış temel mimarilerinden biridir.\n",
    "\n",
    "CNN'lerin (Convolutional Neural Networks) görüntülerdeki mekansal (spatial) ilişkileri yakalamakta uzman olması gibi, RNN'ler de zamansal (temporal) veya sıralı ilişkileri yakalamakta uzmandır.\n",
    "\n",
    "İşte RNN'lerin mantığı, çalışma prensibi ve kullanım alanları:\n",
    "\n",
    "**RNN Nedir ve Neden İhtiyaç Duyarız?**\n",
    "\n",
    "Geleneksel \"İleri Beslemeli\" (Feedforward) sinir ağlarında (örneğin standart bir yoğun katman), girdiler birbirinden bağımsızdır. Örneğin, bir evi fiyatlandırırken, birinci evin özellikleri ikinci evin fiyatını etkilemez.\n",
    "\n",
    "Ancak bazı verilerde sıra çok önemlidir:\n",
    "\n",
    "- Doğal Dil: \"Bu filmi hiç...\" cümlesinin devamı, önceki kelimelere bağlıdır.\n",
    "\n",
    "- Zaman Serileri: Dünün borsa fiyatı, bugünün fiyatını etkileyebilir.\n",
    "\n",
    "- Ses Dalgaları: Bir saniyelik ses, öncesindeki sese bağlıdır.\n",
    "\n",
    "RNN, bu sıralı bilgiyi işlemek için bir hafıza (memory) mekanizmasına sahiptir.\n",
    "\n",
    "**Nasıl Çalışır? (Döngü Mantığı)**\n",
    "\n",
    "RNN'in en ayırt edici özelliği Hidden State (Gizli Durum) adı verilen bir hafıza hücresine sahip olmasıdır.\n",
    "\n",
    "Bir RNN hücresini düşünürken şu süreci hayal edebilirsiniz:\n",
    "\n",
    "1. Ağ, $t$ anındaki girdiyi ($x_t$) alır.\n",
    "\n",
    "2. Aynı zamanda, bir önceki adımdan ($t-1$) gelen hafıza bilgisini ($h_{t-1}$) de alır.\n",
    "\n",
    "3. Bu ikisini birleştirip işleyerek yeni bir çıktı ve yeni bir hafıza durumu ($h_t$) oluşturur.\n",
    "\n",
    "4. Bu yeni hafıza, bir sonraki adıma aktarılır.\n",
    "\n",
    "**Analoji:** Bir kitabı okuduğunuzu düşünün. Her yeni kelimeyi okuduğunuzda (yeni girdi), o anki kelimenin anlamını, o ana kadar okuduğunuz cümlenin bağlamıyla (önceki hafıza) birleştirerek anlarsınız.\n",
    "\n",
    "RNN'in temel formülü oldukça basittir. $t$ zamanındaki yeni gizli durum ($h_t$) şu şekilde hesaplanır:\n",
    "\n",
    "- $$h_t = \\tanh(W \\cdot x_t + U \\cdot h_{t-1} + b)$$\n",
    "\n",
    "- $x_t$: Şu anki girdi (Current Input).\n",
    "\n",
    "- $h_{t-1}$: Önceki gizli durum (Previous Hidden State).\n",
    "\n",
    "- $W, U$: Öğrenilen ağırlık matrisleri.\n",
    "\n",
    "- $\\tanh$: Aktivasyon fonksiyonu (genellikle değerleri -1 ile 1 arasında tutmak için kullanılır).\n",
    "\n",
    "- $b$: Bias terimi.\n",
    "\n",
    "**RNN'in Temel Sorunu: \"Unutkanlık\"**\n",
    "\n",
    "Teoride RNN'ler geçmişteki tüm bilgileri hatırlayabilmelidir. Ancak pratikte Vanishing Gradient (Kaybolan Gradyan) problemi nedeniyle, dizi uzadıkça (örneğin çok uzun bir paragraf) ağ, cümlenin başındaki bilgiyi unutmaya başlar.\n",
    "\n",
    "Buna çözüm olarak daha gelişmiş RNN türevleri geliştirilmiştir:\n",
    "\n",
    "- **LSTM (Long Short-Term Memory):** Bilgiyi ne kadar süre tutacağını veya ne zaman unutacağını seçebilen kapı (gate) mekanizmalarına sahiptir.\n",
    "\n",
    "- **GRU (Gated Recurrent Unit):** LSTM'in daha basitleştirilmiş ve hızlı çalışan bir versiyonudur.\n",
    "\n",
    "| Alan (Domain)            | Somut Görev                            | RNN/LSTM'in Rolü ve İşleyişi                                                                                                                                              | Modern Dönüşüm (Neden LLM Tercih Ediliyor?)                                                                                          |\n",
    "|--------------------------|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Doğal Dil İşleme (NLP)   | Makine Çevirisi (Google Translate - Eski) | Encoder-Decoder Yapısı: Cümleyi kelime kelime okuyup bir \"düşünce vektörüne\" (context vector) sıkıştırır, sonra hedef dile yine kelime kelime çözer.                     | RNN uzun cümlelerde cümlenin başını unutur. Transformer (LLM) ise tüm cümleyi aynı anda görür ve çeviri kalitesi çok daha yüksektir. |\n",
    "| Doğal Dil İşleme (NLP)   | Duygu Analizi (Sentiment Analysis)       | Metni baştan sona okuyarak kelimelerin bağlamını hafızasında tutar ve sonunda \"Olumlu/Olumsuz\" diye tek bir karar verir.                                                  | LLM'ler kelimeler arasındaki nüansları (sarkazm, iğneleme) çok daha iyi yakalar çünkü çok daha geniş bir veriyle eğitilmiştir.        |\n",
    "| Konuşma Tanıma           | Speech-to-Text (Siri, Asistanlar)        | Ses dalgalarını milisaniyelik dilimlere böler. Her dilimi, önceki dilimden gelen ses bilgisiyle birleştirerek harfe veya heceye dönüştürür.                              | OpenAI Whisper gibi modeller artık Transformer tabanlıdır. Gürültülü ortamlarda ve farklı aksanlarda RNN'den çok daha başarılıdır.   |\n",
    "| Zaman Serileri           | Borsa / Talep Tahmini                    | Geçmiş fiyat hareketlerindeki desenleri (yükseliş trendi, mevsimsellik) ezberler ve bir sonraki adımı tahmin eder.                                                        | **Hâlâ Kullanılıyor:** Veri seti küçükse (örneğin sadece 1 yıllık satış verisi), LSTM hala Transformer'dan daha verimli ve hızlı olabilir. |\n",
    "| Müzik & Sanat            | Melodi Üretimi                           | Notaları bir dil gibi görür. \"Do-Re-Mi\" dizisinden sonra \"Fa\" gelme olasılığını hesaplar.                                                                                  | Modern modeller (örn. MusicLM), sadece notayı değil müziğin \"türünü\", \"duygusunu\" ve \"ritmini\" anlayarak bütünsel besteler üretir.    |\n",
    "| Video Analizi            | Aksiyon Tanıma                           | Videoyu kare kare (frame-by-frame) inceler. Bir karedeki hareketin diğer karede nereye gittiğini takip eder.                                                              | Videolarda artık \"Vision Transformers\" (ViT) kullanılmaya başlandı.                                                                   |\n",
    "\n",
    "\n",
    "\n",
    "RNN (Recurrent Neural Network), standart ileri beslemeli (feed-forward) ağlardan farklı olarak, hafızaya sahip bir yapıdır. Bu hafıza, verinin sıralı (sequential) doğasını işlemek için geçmiş bilgiyi (önceki time-step) şu anki girdiye ekler.\n",
    "\n",
    "RNN'in matematiksel \"motorunu\" matris operasyonları seviyesinde parçalayalım.\n",
    "\n",
    "**Temel Yapı ve Notasyon**\n",
    "\n",
    "RNN'in kalbi, Gizli Durum (Hidden State) vektörüdür. Her adımda ($t$), model hem o anki girdiyi ($x_t$) hem de bir önceki adımdan gelen gizli durumu ($h_{t-1}$) alarak yeni bir gizli durum ($h_t$) üretir.\n",
    "\n",
    "*Lütfen dokümana devam etmeden önce videoyu izleyiniz.*\n",
    "\n",
    "[Yinelemeli Sinir Ağlarına Giriş - RNN (Recurrent Neural Network for NLP)](https://www.youtube.com/watch?v=AHybjSLXlV8)\n",
    "\n",
    "**Değişkenlerimiz:**\n",
    "\n",
    "$t$: Zaman adımı (Time step). Örneğin bir cümledeki kelime sırası.\n",
    "\n",
    "- $x_t$: $t$ anındaki Girdi Vektörü. Boyutu: $(d \\times 1)$.\n",
    "\n",
    "- $h_t$: $t$ anındaki Gizli Durum Vektörü (Hafıza). Boyutu: $(h \\times 1)$.\n",
    "\n",
    "- $y_t$: $t$ anındaki Çıktı Vektörü. Boyutu: $(q \\times 1)$.\n",
    "\n",
    "**Ağırlık Matrisleri (Modelin Öğrendiği Parametreler)**\n",
    "\n",
    "Veri akışını sağlayan üç ana ağırlık matrisi vardır:$W_{xh}$ \n",
    "\n",
    "**(Input-to-Hidden):** Girdiyi gizli katmana bağlar. Boyutu: $(h \\times d)$.$W_{hh}$ \n",
    "\n",
    "**(Hidden-to-Hidden):** Önceki hafızayı şimdiki hafızaya bağlar. Boyutu: $(h \\times h)$.$W_{hy}$\n",
    "\n",
    "**(Hidden-to-Output):** Gizli durumdan çıktı üretir. Boyutu: $(q \\times h)$.\n",
    "\n",
    "**Matematiksel Veri Akışı (Forward Pass)**\n",
    "\n",
    "Model her zaman adımında şu iki denklemi sırasıyla çalıştırır:\n",
    "\n",
    "**Adım A: Gizli Durumun Güncellenmesi (Hafıza Yazma)**\n",
    "\n",
    "Yeni gizli durum ($h_t$), önceki hafıza ve yeni girdinin ağırlıklı toplamının, lineer olmayan bir aktivasyon fonksiyonundan (genellikle tanh) geçirilmesiyle bulunur.\n",
    "\n",
    "$$h_t = \\tanh(\\underbrace{W_{hh} \\cdot h_{t-1}}_{\\text{Geçmiş Bilgi}} + \\underbrace{W_{xh} \\cdot x_t}_{\\text{Mevcut Girdi}} + b_h)$$\n",
    "\n",
    "Matris Çarpımı Detayı:\n",
    "\n",
    "$(h \\times h) \\times (h \\times 1) \\rightarrow (h \\times 1)$ boyutunda bir vektör (Geçmiş).\n",
    "\n",
    "$(h \\times d) \\times (d \\times 1) \\rightarrow (h \\times 1)$ boyutunda bir vektör (Girdi).\n",
    "\n",
    "Bu ikisi toplanır, bias ($b_h$) eklenir ve $[-1, 1]$ arasına sıkıştırılır ($\\tanh$).\n",
    "\n",
    "**Adım B: Çıktının Hesaplanması**\n",
    "\n",
    "Eğer her adımda bir çıktı isteniyorsa (örneğin kelime tahminleme), güncel gizli durum ($h_t$) çıkış katmanına gönderilir.\n",
    "\n",
    "$$y_t = W_{hy} \\cdot h_t + b_y$$\n",
    "\n",
    "Çıktı genellikle sınıflandırma için Softmax fonksiyonuna sokulur: $\\hat{y}_t = \\text{softmax}(y_t)$.\n",
    "\n",
    "**Örnek Senaryo: Boyut Analizi**\n",
    "\n",
    "Somutlaştırmak için sayısal bir örnek verelim. Diyelim ki kelime vektörü boyutumuz 100, hafıza (hidden) boyutumuz 256 olsun.\n",
    "\n",
    "1. Girdi ($x_t$): $(100 \\times 1)$ vektör.\n",
    "\n",
    "2. Önceki Hafıza ($h_{t-1}$): $(256 \\times 1)$ vektör.\n",
    "\n",
    "3. Matris İşlemi:\n",
    "\n",
    "- $W_{xh}$ matrisi $(256 \\times 100)$ boyutundadır. $W_{xh} \\cdot x_t$ işlemi $(256 \\times 1)$ sonucunu verir.\n",
    "- $W_{hh}$ matrisi $(256 \\times 256)$ boyutundadır. $W_{hh} \\cdot h_{t-1}$ işlemi $(256 \\times 1)$ sonucunu verir.\n",
    "\n",
    "4. Toplam: İki $(256 \\times 1)$ vektör toplanır $\\rightarrow$ Yeni $h_t$ $(256 \\times 1)$ olur.\n",
    "\n",
    "**Önemli Not:** RNN'lerde bu $W_{hh}, W_{xh}, W_{hy}$ matrisleri tüm zaman adımları (time-steps) boyunca aynıdır (paylaşılır). Yani model kelimenin cümlenin başında veya sonunda olmasından bağımsız olarak, dili işleyen tek bir kural seti (matris) öğrenir.\n",
    "\n",
    "**Neden Artık Saf RNN Kullanılmıyor?**\n",
    "\n",
    "Yukarıdaki $W_{hh} \\cdot h_{t-1}$ işlemi her adımda tekrarlanır. Eğer cümle 100 kelime uzunluğundaysa, geriye doğru türev alırken (Backpropagation) aynı matris kendisiyle 100 kere çarpılır.\n",
    "\n",
    "- Matris değerleri $> 1$ ise: Sayılar sonsuza gider (Exploding Gradient).\n",
    "\n",
    "- Matris değerleri $< 1$ ise: Sayılar sıfıra iner ve model öğrenmeyi durdurur (Vanishing Gradient).\n",
    "\n",
    "Bu yüzden LSTM ve GRU, bu çarpma işlemini \"toplama\" işlemine dönüştüren kapı (gate) mekanizmalarıyla bu matris problemini çözer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9f38d",
   "metadata": {},
   "source": [
    "**Senaryo** \n",
    "\n",
    "Elimizde sadece 2 zaman adımı (time-step) olan bir veri olsun: $x_1$ ve $x_2$.\n",
    "Modelin amacı: $x_1$'i okuyup hafızaya atmak, sonra $x_2$'yi okuyup hem $x_2$'yi hem de hafızadaki $x_1$ bilgisini birleştirmek.\n",
    "\n",
    "Parametrelerimiz (Modelin bildiği sabitler):\n",
    "İşlemleri basitleştirmek için Bias ($b$) değerini 0 kabul edelim ve Ağırlık Matrislerini şu şekilde uyduralım:\n",
    "\n",
    "1. $W_{xh}$ (Girdi Ağırlığı): Birim matris olsun (Girdiyi aynen içeri alır).\n",
    "$$W_{xh} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "2. $W_{hh}$ (Hafıza Ağırlığı): 0.5 ile çarpan bir matris olsun (Geçmişin yarısını hatırla).\n",
    "$$W_{hh} = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{bmatrix}$$\n",
    "\n",
    "3. $h_0$ (Başlangıç Hafızası): İlk başta hafıza boştur.\n",
    "$$h_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "Zaman Adımı 1 ($t=1$): İlk Veri Geliyor\n",
    "Girdi: $x_1 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$\n",
    "Formülümüz neydi?\n",
    "\n",
    "$$h_1 = \\tanh(W_{hh} \\cdot h_0 + W_{xh} \\cdot x_1)$$\n",
    "\n",
    "Matrisleri yerine koyalım:\n",
    "\n",
    "1. Geçmişten gelen: $W_{hh} \\cdot h_0 = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ (Henüz geçmiş yok).\n",
    "\n",
    "2. Şu anki girdi: $W_{xh} \\cdot x_1 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$ (Girdi aynen geçti).\n",
    "\n",
    "3. Toplam: $\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$\n",
    "\n",
    "Sonuç ($h_1$): Aktivasyon fonksiyonu $\\tanh$ bu sayıları -1 ile 1 arasına sıkıştırır (yaklaşık değerler):\n",
    "\n",
    "$$h_1 = \\tanh(\\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}) \\approx \\begin{bmatrix} 0.96 \\\\ 0.99 \\end{bmatrix}$$\n",
    "\n",
    "Özet: İlk adımda model sadece girdiyi aldı, sıkıştırdı ve hafızaya ($h_1$) yazdı.\n",
    "\n",
    "**Zaman Adımı 2 ($t=2$): Kritik Nokta (Hafıza Kullanımı)**\n",
    "\n",
    "Girdi: $x_2 = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}$\n",
    "\n",
    "Formül aynı, ama girdiler değişti:\n",
    "\n",
    "$$h_2 = \\tanh(\\underbrace{W_{hh} \\cdot h_1}_{\\text{Geçmiş}} + \\underbrace{W_{xh} \\cdot x_2}_{\\text{Yeni}})$$\n",
    "\n",
    "İşte \"Recurrent\" (Tekrarlayan) kısım burası. $h_1$'i (önceki sonucu) kullanıyoruz:\n",
    "\n",
    "1. Geçmişten gelen ($h_1$ kullanılıyor!):\n",
    "\n",
    "$$W_{hh} \\cdot h_1 = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.96 \\\\ 0.99 \\end{bmatrix} = \\begin{bmatrix} 0.48 \\\\ 0.495 \\end{bmatrix}$$\n",
    "\n",
    "(Dikkat: Model önceki bilgiyi %50 oranında taşıdı)\n",
    "\n",
    "Şu anki girdi ($x_2$):\n",
    "\n",
    "$$W_{xh} \\cdot x_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}$$\n",
    "\n",
    "Toplam (Geçmiş + Gelecek):\n",
    "\n",
    "$$\\text{Toplam} = \\begin{bmatrix} 0.48 \\\\ 0.495 \\end{bmatrix} + \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} -0.52 \\\\ -0.505 \\end{bmatrix}$$\n",
    "\n",
    "Sonuç ($h_2$):\n",
    "\n",
    "$$h_2 = \\tanh(\\begin{bmatrix} -0.52 \\\\ -0.505 \\end{bmatrix}) \\approx \\begin{bmatrix} -0.47 \\\\ -0.46 \\end{bmatrix}$$\n",
    "\n",
    "**Sonuç Analizi**\n",
    "\n",
    "Eğer bu bir düz (Feed-Forward) ağ olsaydı, 2. adımda sadece $x_2$'yi ([-1, -1]) işleyecekti. Ancak RNN olduğu için, $h_2$ vektörünün içinde $x_1$'den gelen sayısal izler (0.48, 0.495) hala mevcut. İşte veri akışı ve hafıza bu matris toplama işlemiyle sağlanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413b918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Başlangıç Hafızası: [0. 0.]\n",
      "\n",
      "t=1 Sonrası Hafıza (h1): [0.96402758 0.9993293 ]\n",
      "\n",
      "t=2 Sonrası Hafıza (h2): [-0.47614427 -0.46238085]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Aktivasyon fonksiyonu (Tanh)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# 1. Parametreleri Tanımla\n",
    "# Hidden size: 2, Input size: 2\n",
    "W_xh = np.array([[1.0, 0.0], \n",
    "                 [0.0, 1.0]]) # Input ağırlıkları\n",
    "\n",
    "W_hh = np.array([[0.5, 0.0], \n",
    "                 [0.0, 0.5]]) # Hafıza ağırlıkları (Geçmişi 0.5 ile hatırla)\n",
    "\n",
    "# Bias (Basitlik için 0)\n",
    "b_h = np.array([0.0, 0.0])\n",
    "\n",
    "# 2. Veri Seti (Zaman serisi: t1 ve t2)\n",
    "x_1 = np.array([2.0, 4.0])   # t=1 girdisi\n",
    "x_2 = np.array([-1.0, -1.0]) # t=2 girdisi\n",
    "\n",
    "# Başlangıç hafızası (Hidden State)\n",
    "h_prev = np.array([0.0, 0.0]) \n",
    "\n",
    "print(f\"Başlangıç Hafızası: {h_prev}\")\n",
    "\n",
    "# --- ZAMAN ADIMI 1 ---\n",
    "# h1 = tanh(Whh * h0 + Wxh * x1 + b)\n",
    "h_1 = tanh(np.dot(W_hh, h_prev) + np.dot(W_xh, x_1) + b_h)\n",
    "\n",
    "print(f\"\\nt=1 Sonrası Hafıza (h1): {h_1}\")\n",
    "# Çıktı yaklaşık: [0.96, 0.99]\n",
    "\n",
    "# --- ZAMAN ADIMI 2 ---\n",
    "# h2 = tanh(Whh * h1 + Wxh * x2 + b) -> h1 burada girdidir!\n",
    "h_2 = tanh(np.dot(W_hh, h_1) + np.dot(W_xh, x_2) + b_h)\n",
    "\n",
    "print(f\"\\nt=2 Sonrası Hafıza (h2): {h_2}\")\n",
    "# Çıktı yaklaşık: [-0.47, -0.46] \n",
    "# (Dikkat: x2 tek başına -1 iken, h1'den gelen pozitif etki sonucu yukarı çekti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925016e",
   "metadata": {},
   "source": [
    "### \"Eğer bu döngü 100 kere dönseydi, sürekli 0.5 ile çarpmak sayıları sıfırlayacaktı (Vanishing Gradient)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23277eb",
   "metadata": {},
   "source": [
    "**LLM Benzetmesi: \"Kral ve Kraliçe\" Örneği**\n",
    "\n",
    "Diyelim ki modelimiz [Kral, kraliçeye, tacını, verdi] cümlesini işliyor. Bizim önceki matematiksel örneğimizdeki 2 boyutlu vektör yerine, gerçekte 1000 boyutlu bir vektör olduğunu düşünelim. Ancak basitleştirmek için sadece 3 özelliğe odaklanalım:\n",
    "\n",
    "1. Cinsiyet (+1 Erkek, -1 Kadın)\n",
    "\n",
    "2. Asillik (+1 Asil, -1 Halktan)\n",
    "\n",
    "3. Eylem Durumu (+1 Bir şey veriyor, -1 Bir şey alıyor)\n",
    "\n",
    "**Adım 1 ($t=1$): \"Kral\" kelimesi girer**\n",
    "\n",
    "Model \"Kral\" kelimesini matematiksel vektöre dönüştürür.\n",
    "\n",
    "- Vektör ($x_1$): [+0.99, +0.99, 0.0]\n",
    "\n",
    "- Anlamı: Bu kişi Erkek, bu kişi Asil, şu an bir eylem yok.\n",
    "\n",
    "- Hafıza ($h_1$): Model hafızasına şunu kazır: \"Konumuz asil bir erkekle ilgili.\"\n",
    "\n",
    "**Adım 2 ($t=2$): \"Kraliçeye\" kelimesi girer**\n",
    "\n",
    "İşte RNN'in büyüsü (matris çarpımı) burada devreye girer.\n",
    "\n",
    "- Girdi ($x_2$): [-0.99, +0.99, 0.0] (Kadın, Asil).\n",
    "\n",
    "- Önceki Hafıza ($h_1$): (Erkek, Asil).\n",
    "\n",
    "Model bu ikisini topladığında (matris işlemi), hafıza vektörü ($h_2$) karışık bir hal alır:\n",
    "\n",
    "- Cinsiyet boyutu: Erkek + Kadın $\\rightarrow$ Nötrleşebilir veya cümleye göre değişir.\n",
    "\n",
    "- Asillik boyutu: Asil + Asil $\\rightarrow$ +1.98 (Çok Güçlü Asillik).\n",
    "\n",
    "Sonuç: Modelin $h_2$ anındaki hafızası artık şunu der: \"Burada çok asil bir ortam var (Saraydayız), iki kişi var.\"\n",
    "\n",
    "**Adım 3 ($t=3$): \"Tacını\" kelimesi girer**\n",
    "\n",
    "Eğer modelin bir sonraki kelimeyi (LLM gibi) tahmin etmesi gerekirse, \"Asillik\" değeri tavan yaptığı için \"elma\" veya \"araba\" değil, asillikle ilgili bir nesne (taht, asa, taç) tahmin etme olasılığı artar.\n",
    "\n",
    "Özet: O gördüğün sayılar, kelimenin \"anlam koordinatlarıdır\". Sayıların toplanması demek, anlamların üst üste binip hikayenin bağlamını oluşturması demektir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8e7c1",
   "metadata": {},
   "source": [
    "**Uygulama Benzetmesi: Film Eleştirmeni (Duygu Analizi)**\n",
    "\n",
    "Bir film sitesi için yorumları okuyup \"Olumlu\" veya \"Olumsuz\" diyen bir yapay zeka düşünelim.\n",
    "Tek bir sayıya (skor) odaklanalım. ($-1$: Nefret, $+1$: Sevgi).\n",
    "\n",
    "Cümle: \"Film harikaydı ama sonu berbattı.\"\n",
    "\n",
    "1. \"Film\" ($t=1$): Nötr. Skor: 0.1 (Henüz bir şey yok).\n",
    "\n",
    "2. \"Harikaydı\" ($t=2$): Çok pozitif bir kelime.\n",
    "\n",
    "- Matematiksel işlem: Önceki 0.1 + Yeni 0.9 = 1.0.\n",
    "\n",
    "- Hafıza Durumu ($h_2$): \"Şu ana kadar seyirci çok mutlu.\" (+1.0)\n",
    "\n",
    "3. \"Ama\" ($t=3$): Kritik kelime!\n",
    "\n",
    "- RNN bunu öğrendiyse, \"ama\" kelimesinin negatif çarpan etkisi olabilir veya mevcut hafızayı zayıflatabilir.\n",
    "\n",
    "- Hafıza ($h_3$) 1.0'dan 0.5'e düşebilir. (Dikkatli ol, ters köşe geliyor sinyali).\n",
    "\n",
    "4. \"Sonu\" ($t=4$): Nötr. Skor hala 0.5.\n",
    "\n",
    "5. \"Berbattı\" ($t=5$): Çok negatif (-0.9).\n",
    "\n",
    "- İşlem: Hafızadaki 0.5 (önceki olumluluk) ile -0.9 (şimdiki nefret) toplanır.\n",
    "\n",
    "- Sonuç: -0.4\n",
    "\n",
    "Final Karar: Modelin son çıktısı negatiftir. Eğer RNN hafızayı (önceki time-step'leri) tutmasaydı, sadece son kelimeye baksaydı \"berbat\" deyip geçerdi. Ama hafıza sayesinde \"başta iyiydi ama sonra bozdu\" nüansını (matematiksel toplamla) yakalayabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc63994",
   "metadata": {},
   "source": [
    "*\"Matematiksel olarak yaptığımız matris çarpımları, aslında insan beynindeki 'kısa süreli hafıza' gibidir. Bir cümleyi okurken cümlenin başını aklımızda tutmamız (Hidden State), cümlenin sonunu anlamlandırmamızı sağlar. RNN'deki sayılar, bu bilginin beyindeki elektrik sinyalleri gibi dijital karşılığıdır.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0833e0",
   "metadata": {},
   "source": [
    "*Lütfen dokümana devam etmeden önce aşağıdaki videoyu izleyiniz*\n",
    "\n",
    "[RNN Genel Mimari](https://www.youtube.com/watch?v=uWiBr1YXtWY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c5c0a",
   "metadata": {},
   "source": [
    "RNN'in \"unutkanlık\" hastalığını tedavi eden modern mimarilere, LSTM ve GRU'ya geçiyoruz.\n",
    "\n",
    "Önceki bölümde gördüğümüz sorun şuydu: RNN, matrisleri sürekli birbiriyle çarptığı için (çarpma işlemi), uzun cümlelerde bilgi ya patlıyor ya sönümleniyordu\n",
    "\n",
    "Çözüm? Çarpma yerine \"Toplama\" işlemini ana yol haline getirmek.\n",
    "\n",
    "**LSTM (Long Short-Term Memory) - \"Mühendislik Harikası\"**\n",
    "\n",
    "LSTM'i anlamanın en kolay yolu, onu bir fabrikadaki taşıma bandı (conveyor belt) gibi düşünmektir.\n",
    "\n",
    "RNN'de sadece bir \"Gizli Durum\" ($h_t$) vardı ve bu sürekli değişiyordu. LSTM'de ise iki yol vardır:\n",
    "\n",
    "1. Hücre Durumu / Cell State ($C_t$): Bu, verinin aktığı \"Otoban\"dır. Bilgi burada çok az değişikliğe uğrayarak dümdüz akar (Burası hafızanın korunduğu yerdir).\n",
    "\n",
    "2. Gizli Durum / Hidden State ($h_t$): Bu ise o anki kararları veren \"Çalışan\"dır.\n",
    "\n",
    "Matematiksel Akış ve \"Kapı\" (Gate) Mantığı\n",
    "\n",
    "LSTM, \"Kapı\" adı verilen yapılarla bilgi akışını kontrol eder. Kapılar, Sigmoid fonksiyonuyla çalışır.\n",
    "\n",
    "- Sigmoid Çıktısı 0: Vanayı kapat (Bilgiyi geçirme/unut).\n",
    "\n",
    "- Sigmoid Çıktısı 1: Vanayı aç (Bilgiyi olduğu gibi geçir).\n",
    "\n",
    "LSTM'de veri akışı adım adım şöyledir:\n",
    "\n",
    "**A. Unutma Kapısı (Forget Gate):** \"Eski bilgiyi sileyim mi?\"\n",
    "Model, eski hafızaya ($C_{t-1}$) bakar ve yeni girdiye ($x_t$) bakar.\n",
    "\n",
    "- Örnek: \"Ahmet iyi bir adamdı... (yeni cümle) ...Ayşe ise...\"\n",
    "\n",
    "- Karar: Cinsiyet bilgisini \"Erkek\"ten \"Kadın\"a çevirmem lazım. Eski \"Erkek\" bilgisini unut ($0$ ile çarp).\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t])$$\n",
    "\n",
    "**B. Giriş Kapısı (Input Gate):** \"Yeni bilgiyi ekleyeyim mi?\" Yeni gelen bilginin ne kadar önemli olduğuna karar verilir.\n",
    "\n",
    "- Karar: \"Ayşe\" bilgisini hafızaya ekle ($1$ ile çarp).\n",
    "\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t])$$\n",
    "\n",
    "**C. HÜCRE GÜNCELLEME (En Kritik Adım)**\n",
    "İşte büyü buradaki Toplama ($+$) işlemindedir:\n",
    "\n",
    "$$C_t = \\underbrace{(f_t \\cdot C_{t-1})}_{\\text{Eskiyi Unut/Hatırla}} + \\underbrace{(i_t \\cdot \\tilde{C}_t)}_{\\text{Yeniyi Ekle}}$$\n",
    "\n",
    "Neden Çalışıyor? RNN'de sürekli çarpma vardı ($W \\cdot h$). Burada ise Toplama var. Türev alırken toplama işlemi gradyanı (bilgiyi) çok daha uzun mesafelere, bozulmadan taşır. Otobandaki araba gibi, bilgi $C_t$ hattında kaybolmadan ilerler.\n",
    "\n",
    "D. Çıkış Kapısı (Output Gate): \"Dış dünyaya ne söyleyeyim?\"\n",
    "Hafızadaki her şeyi dışarı yansıtmaz, sadece o an gerekli olanı ($h_t$) filtreleyip dışarı verir.\n",
    "\n",
    "**GRU (Gated Recurrent Unit) - \"Verimli Kardeş\"**\n",
    "\n",
    "LSTM çok güçlüdür ama çok fazla parametresi (4 farklı sinir ağı katmanı) vardır. Hesaplama maliyeti yüksektir. GRU, \"Hücre Durumu\" ve \"Gizli Durumu\" tek bir vektörde birleştirerek LSTM'i sadeleştirir.\n",
    "\n",
    "GRU'da 3 yerine sadece 2 kapı vardır:\n",
    "\n",
    "1. Güncelleme Kapısı (Update Gate - $z_t$): \"Geçmişten ne kadar bilgiyi taşıyayım?\" (LSTM'deki Unutma ve Giriş kapılarının birleşimi gibidir).\n",
    "\n",
    "2. Sıfırlama Kapısı (Reset Gate - $r_t$): \"Geçmiş bilgiyi ne kadar ihmal edeyim?\"\n",
    "\n",
    "**Matematiksel Fark:**\n",
    "$$h_t = (1 - z_t) \\cdot h_{t-1} + z_t \\cdot \\tilde{h}_t$$\n",
    "\n",
    "Bu formül şu demektir:\n",
    "\n",
    "- Eğer $z_t \\approx 1$ ise: Geçmişi ($h_{t-1}$) tamamen unut, sadece yeni bilgiye ($\\tilde{h}_t$) odaklan.\n",
    "\n",
    "- Eğer $z_t \\approx 0$ ise: Yeni bilgiyi yoksay, geçmişi aynen koru.\n",
    "\n",
    "Senaryo: Bir öğrenci (Model) final sınavına çalışıyor. Konular sırayla geliyor (Veri akışı).\n",
    "\n",
    "Öğrenci her yeni konuyu duyduğunda, beynindeki tüm bilgiyi o anki duyduğuyla karıştırıp yeniden yazar.\n",
    "\n",
    "Sorun: 10. konuya geldiğinde 1. konuyu hatırlaması imkansızdır, çünkü 9 kere üzerine yazmıştır. (Vanishing Gradient).\n",
    "\n",
    "**LSTM (Mühendis Öğrenci)**\n",
    "\n",
    "Öğrencinin bir \"Not Defteri\" (Cell State) vardır.\n",
    "\n",
    "1. Forget Gate: \"Hoca 'Bu konu sınavda çıkmayacak' dedi.\" $\\rightarrow$ O sayfayı yırt at (0 ile çarp).\n",
    "\n",
    "2. Input Gate: \"Hoca 'Bu formül çok önemli' dedi.\" $\\rightarrow$ Deftere büyük harflerle yaz (Ekle).\n",
    "\n",
    "3. Output Gate: Sınav kağıdına ne yazacağım? $\\rightarrow$ Defterdeki her şeyi değil, sadece soruyla ilgili kısmı yaz.\n",
    "\n",
    "Sonuç: Dönem başındaki notu (uzun vadeli hafıza) dönem sonunda hala defterde bozulmadan durur.\n",
    "\n",
    "**GRU (Pratik Öğrenci)**\n",
    "\n",
    "Not defteri yoktur, her şeyi kafasında tutar ama çok zeki bir filtreleme sistemi vardır.\n",
    "\n",
    "- \"Bu yeni bilgi, bildiğim eski bilgiyle çelişiyor mu? Çelişiyorsa eskisini sil, yenisini koy. Çelişmiyorsa ikisini birleştir.\"\n",
    "\n",
    "- Daha hızlı karar verir, daha az enerji harcar.\n",
    "\n",
    "**Hangisini Seçmeli?**\n",
    "\n",
    "| Özellik         | LSTM                                      | GRU                                  |\n",
    "|-----------------|-------------------------------------------|-------------------------------------|\n",
    "| Karmaşıklık     | Yüksek (Daha çok matris)                 | Düşük (Daha az matris)              |\n",
    "| Eğitim Hızı     | Yavaş                                     | Hızlı                               |\n",
    "| Veri Miktarı    | Çok büyük veri setlerinde genelde daha iyi| Az/Orta veri setlerinde harika çalışır |\n",
    "| Uzun Bağlam     | Çok uzun cümlelerde/geçmişte çok iyidir  | Biraz daha kısa bağlamlarda iyidir  |\n",
    "\n",
    "**\"Sıralılık Darboğazı\" (Sequential Bottleneck)**\n",
    "\n",
    "**RNN/LSTM Mantığı (Bayrak Yarışı):** 100 kelimelik bir cümleniz varsa, 100. kelimeyi işlemek için 99. kelimenin bitmesini beklemek zorundasınız. Veri sırayla akar. Bu durum, modern GPU'ların (ekran kartlarının) paralel işlem gücünü kullanmayı engeller. Çok yavaştır.\n",
    "\n",
    "**İletişim Kopukluğu:** Cümlenin başındaki kelime ile sonundaki kelime arasında ilişki kurmak için, bilgi 100 adım boyunca taşınmalı. LSTM bunu yapsa da yol uzundur.\n",
    "\n",
    "**Transformer Devrimi:** \"Neden sırayla gidiyoruz ki? Cümlenin tamamını aynı anda (paralel) içeri atalım ve her kelime diğer tüm kelimelere aynı anda baksın.\"\n",
    "\n",
    "**İşte bu \"herkesin herkese aynı anda bakması\" olayına Self-Attention (Öz-Dikkat) denir.**\n",
    "\n",
    "### *Yeni konuya geçmeden önce konuyu pekiştirmek için alakalı görevleri yapabilirsiniz*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd117cd",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402858f",
   "metadata": {},
   "source": [
    "**Transformer & Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f93d0",
   "metadata": {},
   "source": [
    "Google araştırmacılarının 2017'de yayınladığı \"Attention Is All You Need\" makalesiyle ortaya çıktı. RNN'in sıralı mantığını çöpe atıp, paralel işlemeyi (her şeye aynı anda bakmayı) getirdi.\n",
    "\n",
    "![scatter plot](https://i.imgur.com/VZWdaWN.jpeg)\n",
    "\n",
    "\n",
    "Token, Büyük Dil Modelleri (LLM - Large Language Models) dünyasında, modelin metni işlemek ve anlamlandırmak için kullandığı en küçük veri birimidir.\n",
    "\n",
    "LLM'ler (GPT-4, Claude, Llama vb.) metinleri bizim gibi kelimeler veya cümleler olarak okumazlar. Bunun yerine, metni sayısal dizilere dönüştürürler. İşte metnin sayıya dönüştürülmeden önceki \"parçalanmış\" haline token denir.\n",
    "\n",
    "Bir token; bir kelimenin tamamı, bir kelimenin parçası (hece), bir harf veya hatta bir boşluk karakteri olabilir.\n",
    "\n",
    "- Yaygın İngilizce kelimeler genellikle tek bir tokendir (örneğin: \"apple\").\n",
    "\n",
    "- Karmaşık veya birleşik kelimeler birden fazla tokena bölünür.\n",
    "\n",
    "- Türkçe gibi sondan eklemeli dillerde, tek bir kelime birçok ek aldığı için birden fazla tokena bölünebilir.\n",
    "\n",
    "Analoji: Tokenları LEGO parçaları gibi düşünebilirsiniz. Bir cümle inşa etmek için kelimeleri değil, bu küçük yapı taşlarını (heceleri/parçaları) birleştirir model.\n",
    "\n",
    "**Genel bir kural olarak:**\n",
    "\n",
    "- İngilizce'de: 1000 token yaklaşık 750 kelimeye eşittir. (1 token $\\approx$ 0.75 kelime).\n",
    "\n",
    "- Türkçe'de: Türkçe sondan eklemeli olduğu için (örn: yap-a-ma-y-abil-ir-im), 1 kelime bazen 3-4 token kaplayabilir. Bu, Türkçe kullanımının token limitlerini daha hızlı doldurmasına neden olabilir.\n",
    "\n",
    "**Örnek:** \"Yapay zeka\" ifadesi tokenlara şöyle ayrılabilir (Modele göre değişir):\n",
    "\n",
    "Yapay (Token ID: 4521)\n",
    "\n",
    "zeka (Token ID: 8912)\n",
    "\n",
    "Ancak \"Güzelleştiremediklerimizden\" gibi uzun bir kelime muhtemelen şöyle parçalanır: \n",
    "\n",
    "- Güzel + leş + tire + medik + lerimiz + den\n",
    "\n",
    "[Ders 3: Sıfırdan Python ile Tokenizer Kodlama](https://www.youtube.com/watch?v=X87fSSHJvaA&list=PLrWGe5fM0LZ6Y_C4ka8s8RMA5UoN_CgHT&index=4)\n",
    "\n",
    "*Yukarıdaki linkte Ders-3 ve Ders-7 token kavramını uygulamalı olarak anlatmakta, ödev değildir fakat bu video serisini takip etmeniz faydanıza olacaktır.*\n",
    "\n",
    "**Bilgisayar Tokenları Nasıl Görür?**\n",
    "\n",
    "LLM'ler kelimeleri anlamaz, matematiği anlar. Süreç şöyle işler:\n",
    "\n",
    "- Metin Girişi: \"Merhaba Dünya\"\n",
    "\n",
    "- Tokenizasyon: [Merhaba] [ Dünya]\n",
    "\n",
    "- ID Atama: Her tokenın sözlükte benzersiz bir sayısı vardır. Örn: [1542, 3921]\n",
    "\n",
    "- Vektörleştirme (Embedding): Bu sayılar, modelin işleyebileceği çok boyutlu sayı dizilerine (vektörlere) dönüştürülür.\n",
    "\n",
    "**Neden Önemlidir?**\n",
    "\n",
    "- Maliyet: API (OpenAI, Anthropic vb.) kullanıyorsanız, ücretlendirme \"kelime\" başına değil, \"token\" başına yapılır.\n",
    "\n",
    "- Context Window (Bağlam Penceresi): Her modelin hatırlayabileceği bir sınır vardır (örneğin 128k token). Bu sınır dolduğunda model konuşmanın en başını \"unutur\". Türkçe konuştuğunuzda bu sınır İngilizceye göre daha çabuk dolabilir.\n",
    "\n",
    "| Kavram         | Açıklama                                                                 |\n",
    "|----------------|-------------------------------------------------------------------------|\n",
    "| Token          | Modelin işlediği metin parçası (hece, kelime veya harf).                |\n",
    "| Tokenizer      | Metni tokenlara bölen ve sayıya çeviren algoritma (örn. BPE - Byte Pair Encoding). |\n",
    "| Context Window | Modelin aynı anda hafızasında tutabileceği maksimum token sayısı.       |\n",
    "\n",
    "1. **Sorun: Token ID'leri Yetersizdir**\n",
    "\n",
    "Az önce öğrendik ki Kedi kelimesi Token ID: 45, Köpek kelimesi Token ID: 9852 olabilir. Bilgisayar için 45 ile 9852 arasında hiçbir ilişki yoktur. Biri diğerinden sadece büyüktür. Ama biz biliyoruz ki \"Kedi\" ve \"Köpek\" birbirine çok benzer (ikisi de evcil hayvan).\n",
    "\n",
    "Bilgisayara bu kelimelerin benzer olduğunu nasıl anlatırız?\n",
    "\n",
    "2. **Çözüm: Kelimeleri Koordinatlara Çevirmek**\n",
    "\n",
    "Her kelimeyi devasa bir haritadaki bir nokta gibi düşünün. Benzer kelimeler bu haritada birbirine çok yakın, alakasız kelimeler ise çok uzak durmalıdır.\n",
    "\n",
    "Bir kelimeyi tek bir sayı (ID) ile değil, bir sayı listesiyle (vektör) ifade ederiz.\n",
    "\n",
    "- Kedi: [0.9, 0.1, -0.5]\n",
    "\n",
    "- Köpek: [0.8, 0.2, -0.4] (Kediye sayısal olarak çok yakın)\n",
    "\n",
    "- Masa: [-0.9, 0.8, 0.1] (Kedi ve köpekten sayısal olarak çok uzak)\n",
    "\n",
    "İşte bu sayı listesine Vektör, bu işleme de Embedding denir.\n",
    "\n",
    "[Ders 8: PyTorch ile DataLoader Oluşturma](https://www.youtube.com/watch?v=z8nQeU7eO_A&list=PLrWGe5fM0LZ6Y_C4ka8s8RMA5UoN_CgHT&index=9)\n",
    "\n",
    "*Yukarıdaki linkte Ders-8 ve Ders-13 embedding ve encoding kavramlarını anlatmakta, ödev değildir fakat bu video serisini takip etmeniz faydanıza olacaktır.*\n",
    "\n",
    "3. **Transformer Blokları (Katmanlar)**\n",
    "Modelin \"beyni\" olan kısımdır. Veri burada bir dizi işlemden geçer. Bu bloklar iki ana parçadan oluşur:\n",
    "\n",
    "*Lütfen dokümana devam etmeden önce aşağıdaki videoları izleyiniz*\n",
    "\n",
    "[Transformers, the tech behind LLMs | Deep Learning Chapter 5](https://www.youtube.com/watch?v=wjZofJX0v4M)\n",
    "\n",
    "[Attention in transformers, step-by-step | Deep Learning Chapter 6](https://www.youtube.com/watch?v=eMlx5fFNoYc)\n",
    "\n",
    "[How might LLMs store facts | Deep Learning Chapter 7](https://www.youtube.com/watch?v=9-Jl0dxWQs8)\n",
    "\n",
    "[Transformer #nedir ?](https://www.youtube.com/watch?v=-ec_vIcrJ_0)\n",
    "\n",
    "**A. Çok Başlı Öz-Dikkat Mekanizması (Multi-Head Self-Attention)**\n",
    "\n",
    "Bu katman, modelin kelimeler arasındaki ilişkileri kurduğu yerdir. Bir cümleyi okurken hangi kelimenin diğer hangi kelimelerle daha ilişkili olduğunu hesaplar.\n",
    "\n",
    "- Nasıl Çalışır? (Analoji): Bir sınıfta olduğunuzu düşünün. Öğretmen \"Ali, kalemini Ayşe'ye verdi\" dediğinde; \"verdi\" eyleminin kiminle (Ali) ve neyle (kalem) ilgili olduğunu anlamanız gerekir. Self-Attention mekanizması, cümledeki her kelimenin diğer tüm kelimelere ne kadar \"dikkat etmesi\" gerektiğini puanlar.\n",
    "\n",
    "- Teknik Detay (Q, K, V): Bu mekanizma üç vektör üzerinden işler:\n",
    "\n",
    "1. Query (Sorgu): Aradığımız bilgi (Örn: \"Benimle ilgili kim var?\").\n",
    "\n",
    "2. Key (Anahtar): İçeriğin etiketi veya kimliği.\n",
    "\n",
    "3. Value (Değer): İçeriğin kendisi/anlamı.\n",
    "\n",
    "Dikkat mekanizmasının matematiksel formülü şöyledir:\n",
    "\n",
    "$$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "- **Neden \"Çok Başlı\" (Multi-Head)?** Tek bir dikkat mekanizması cümlenin sadece bir yönüne (örneğin sadece gramer yapısına) odaklanabilir. \"Multi-Head\" sayesinde model, cümlenin farklı özelliklerine (biri gramere, biri anlamsal ilişkiye, biri mesafeye) aynı anda odaklanabilir. Tıpkı bir metni aynı anda 8 farklı uzmanın okuyup yorumlaması gibidir.\n",
    "\n",
    "**B. Konum Bazlı İleri Beslemeli Ağlar (Position-wise Feed-Forward Networks)**\n",
    "\n",
    "Dikkat katmanından gelen bilgiler, kelimelerin birbirleriyle olan ilişkilerini içerir. Ancak bu bilginin işlenmesi ve daha karmaşık özelliklerin çıkarılması gerekir.\n",
    "\n",
    "İşlevi: Attention katmanından çıkan vektörleri alır, bunları genişletir, bir aktivasyon fonksiyonundan (genellikle ReLU veya GELU) geçirir ve tekrar eski boyutuna indirger.\n",
    "\n",
    "Önemi: Burası modelin \"ezber\" veya \"bilgi işleme\" kapasitesinin büyük kısmının yattığı yerdir. Dikkat mekanizması \"nereye bakacağını\" söylerken, Feed-Forward katmanı \"baktığı şeyin ne anlama geldiğini\" yorumlar.\n",
    "\n",
    "**C. Destekleyici Mekanizmalar: Artık Bağlantılar ve Normalizasyon (Add & Norm)**\n",
    "\n",
    "Bu iki ana parçanın (Attention ve Feed-Forward) etrafında hayati öneme sahip iki küçük işlem daha vardır:\n",
    "\n",
    "**Artık Bağlantılar (Residual Connections):** Girdinin, işlemden geçmeden doğrudan çıktının üzerine eklenmesidir\n",
    "\n",
    "- Neden? Derin ağlarda bilginin kaybolmasını (vanishing gradient) önler. Bilgi için bir \"otoban\" sağlar.\n",
    "\n",
    "**Katman Normalizasyonu (Layer Normalization):**\n",
    "\n",
    "- Neden? Verilerin sayısal değerlerini belli bir aralıkta tutarak eğitim sürecinin istikrarlı ve hızlı olmasını sağlar.\n",
    "\n",
    "**Özetle İşleyiş Sırası**\n",
    "\n",
    "Bir kelime vektörü Transformer bloğuna girdiğinde şu yolu izler:\n",
    "\n",
    "1. Dikkat: Önce diğer kelimelerle ilişkisine bakılır (Attention).\n",
    "\n",
    "2. Topla ve Normalleştir: Eski bilgisi ile yeni ilişkisel bilgisi harmanlanır (Add & Norm).\n",
    "\n",
    "3. İşle: Harmanlanan bilgi derinlemesine işlenir (Feed-Forward).\n",
    "\n",
    "4. Topla ve Normalleştir: Sonuç tekrar stabilize edilir ve bir sonraki bloğa gönderilir (Add & Norm).\n",
    "\n",
    "&\n",
    "\n",
    "**Transformer'ı Oluşturan 5 Temel Bileşen**\n",
    "\n",
    "**1. Attention (Dikkat Mekanizması)**\n",
    "\n",
    "**Görevi:** \"Bağlamı kurmak.\"\n",
    "\n",
    "**Açıklama:** Modelin kelimeler arasındaki ilişkiyi kurduğu, \"Hangi kelime hangisiyle ilgili?\" sorusunu çözdüğü yerdir. Veri buraya girdiğinde ham haldedir, çıktığında ise bağlamsal bilgiyle zenginleşmiştir.\n",
    "\n",
    "**2. Layer Norm (Katman Normalizasyonu)**\n",
    "\n",
    "**Görevi:** \"Dengeyi sağlamak.\"\n",
    "\n",
    "**Açıklama:** Matematiksel işlemler sırasında sayılar bazen çok büyüyebilir veya çok küçülebilir (kararsızlık). Layer Norm, veriyi standart bir aralığa çekerek modelin \"kafasının karışmasını\" engeller ve öğrenmeyi hızlandırır.\n",
    "\n",
    "**Analoji:** Bir ses kaydındaki cızırtıları temizleyip ses seviyesini dengelemek gibidir.\n",
    "\n",
    "**4. Residual Connections (Ekleme / Add)**\n",
    "\n",
    "\"Add & Norm\" olarak geçer.\n",
    "\n",
    "**Görevi:** Bilginin kaybolmasını önlemek.\n",
    "\n",
    "Giren verinin orijinal halini, işlenmiş verinin üzerine ekler. Böylece model derinleştikçe (katman sayısı arttıkça) orijinal bilgi unutulmaz.\n",
    "\n",
    "**5. Softmax ve Output (Çıktı Katmanı)**\n",
    "\n",
    "**Görevi:** \"Karar vermek ve Olasılık Hesaplamak.\"\n",
    "\n",
    "**Yeri:** Bu parça genellikle her bloğun içinde değil, modelin en sonunda bir kez bulunur.\n",
    "\n",
    "**Açıklama:** Modelin ürettiği sayısal vektörleri (logits), insan tarafından anlaşılabilir olasılıklara dönüştürür. Kelime dağarcığındaki tüm kelimeler için bir yüzde verir (Örn: %80 'kedi', %15 'köpek', %5 'masa'). En yüksek olasılığa sahip olan, \"Output\" (Çıktı) olarak seçilir.\n",
    "\n",
    "*\"Veri önce **Attention** ile bağlamını bulur, **Layer Norm** ile dengelenir, **MLP** ile işlenip zenginleştirilir ve en sonunda **Softmax** ile bir sonraki kelimenin ne olacağına karar verilir.\"*\n",
    "\n",
    "[Ders 14: Modelin Temelini Oluşturmak](https://www.youtube.com/watch?v=lxj4HY74df4&list=PLrWGe5fM0LZ6Y_C4ka8s8RMA5UoN_CgHT&index=15)\n",
    "\n",
    "\n",
    "*Ders 14'ten itibaren artık Transformer mimarisi ile LLM oluşturma ve playlist'e ait projeye ulaşabilirsiniz. Bu playlistten yararlanmayı lütfen unutmayın, ayriyeten önceki haftanın dokümanında kaynaklar kısmında bulunan LLM 'i çok güzel anlatan LLM 'in öncüleri olan isimlerin de bu konu hakkında neler düşündüklerini izlemeyi unutmayın.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f5fc4",
   "metadata": {},
   "source": [
    "**BERT vs GPT**\n",
    "\n",
    "Bu iki model de aynı aileden (Transformer) gelseler de, eğitim şekilleri yüzünden yetenekleri tamamen zıttır:\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers):**\n",
    "\n",
    "- Görevi: Anlamak (NLU - Natural Language Understanding).\n",
    "\n",
    "- Analoji: BERT, eline bir kitap alıp tüm sayfayı aynı anda okuyan, kelimelerin sağındaki ve solundaki bağlama bakarak anlamı çözen \"dikkatli bir okuyucudur\".\n",
    "\n",
    "- Nasıl Çalışır? Cümlenin ortasındaki bir kelimeyi gizler (Masking) ve \"Cümlenin geri kalanına bakarak bu gizli kelime ne olmalı?\" diye tahmin etmeye çalışır.\n",
    "\n",
    "**GPT (Generative Pre-trained Transformer):**\n",
    "\n",
    "- Görevi: Üretmek (NLG - Natural Language Generation).\n",
    "\n",
    "- Analoji: GPT, kalemi eline almış, bir cümleye başlayıp bir sonraki kelimenin ne olacağını tahmin ederek yazmaya devam eden \"yaratıcı bir yazardır\".\n",
    "\n",
    "- Nasıl Çalışır? Geleceği görmez. Sadece o ana kadar yazılanlara bakar ve \"Sırada gelmesi gereken en mantıklı kelime nedir?\" diye tahmin eder.\n",
    "\n",
    "[GPT vs BERT Explained : Transformer Variations & Use Cases Simplified](https://www.youtube.com/watch?v=AprUD-TSUYE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a49aa",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8fa07",
   "metadata": {},
   "source": [
    "**LLM Modelling & Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f74f7",
   "metadata": {},
   "source": [
    "*Aşağıdaki LLM Visualization aracını kullanarak tüm adımların özetini çıkarınız*\n",
    "\n",
    "[LLM Visualization](https://bbycroft.net/llm) $\\rightarrow$ *Web Sitesi*\n",
    "\n",
    "[LLM Visualization for Understanding - Fahd Mirza](https://www.youtube.com/watch?v=sB2v_7G2sVs) \n",
    "\n",
    "[How does an LLM ACTUALLY Work? (Visual Breakdown)](https://www.youtube.com/watch?v=tNXVYVMME1I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa986643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"https://bbycroft.net/llm\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x27fcd6beb70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='https://bbycroft.net/llm', width='100%', height='800px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f195120",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484d881",
   "metadata": {},
   "source": [
    "**Değerlendirme ve Görevler**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d96ff7",
   "metadata": {},
   "source": [
    "[PyTorch RNN Tutorial - Name Classification Using A Recurrent Neural Net](https://www.youtube.com/watch?v=WEV61GmmPrk)\n",
    "\n",
    "*Videodaki projeyi yapınız*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Classification (Char-RNN) — Quick demo\n",
    "# Küçük bir karakter-seviyeli RNN ile isimleri (erkek/kadın) sınıflandırma örneği.\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Tiny synthetic dataset\n",
    "names = [\n",
    "    (\"Emma\", 0), (\"Olivia\", 0), (\"Ava\", 0), (\"Isabella\", 0), (\"Sophia\", 0),\n",
    "    (\"Liam\", 1), (\"Noah\", 1), (\"Oliver\", 1), (\"Elijah\", 1), (\"James\", 1)\n",
    "]\n",
    "\n",
    "# build char vocabulary\n",
    "chars = sorted(list({c.lower() for n,_ in names for c in n}))\n",
    "stoi = {c:i+1 for i,c in enumerate(chars)}  # 0 reserved for padding\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "vocab_size = len(stoi)+1\n",
    "\n",
    "max_len = max(len(n) for n,_ in names)\n",
    "\n",
    "def encode(name):\n",
    "    arr = [stoi[c.lower()] for c in name]\n",
    "    # pad\n",
    "    arr = arr + [0]*(max_len - len(arr))\n",
    "    return torch.tensor(arr, dtype=torch.long)\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.data = [(encode(n), torch.tensor(lbl, dtype=torch.long)) for n,lbl in pairs]\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.data[idx]\n",
    "\n",
    "ds = NameDataset(names)\n",
    "loader = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# Model: simple embedding + RNN (GRU) + classifier\n",
    "class SimpleNameRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb=16, hidden=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, x):\n",
    "        e = self.embed(x)\n",
    "        out, h = self.gru(e)\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNameRNN(vocab_size).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# Quick training\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total=0; correct=0; running_loss=0.0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = crit(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        pred = logits.argmax(dim=1)\n",
    "        total += yb.size(0); correct += (pred==yb).sum().item()\n",
    "    print(f\"Epoch {epoch+1}/5 — loss: {running_loss/len(loader):.4f} — acc: {correct/total:.2f}\")\n",
    "\n",
    "# Save\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "path = './models/name_rnn_quick.pth'\n",
    "torch.save({'model_state':model.state_dict(),'stoi':stoi,'max_len':max_len}, path)\n",
    "print('Saved model to', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6029e2",
   "metadata": {},
   "source": [
    "[PyTorch Tutorial - RNN & LSTM & GRU - Recurrent Neural Nets](https://www.youtube.com/watch?v=0_PgWWmauHk)\n",
    "\n",
    "*Videodaki projeyi yapınız*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN vs LSTM vs GRU — tiny sequence modeling demo\n",
    "# Task: predict next value in a short numeric sequence (toy task) and show one training step per model.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# synthetic sequences: input seq of length 5, target = next value (sum of inputs mod 10)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "X = np.random.randint(0,10,(100,5)).astype(np.float32)/10.0\n",
    "Y = (X.sum(axis=1) % 1.0).astype(np.float32)  # small float target\n",
    "\n",
    "X = torch.tensor(X).unsqueeze(-1)  # (N,5,1)\n",
    "Y = torch.tensor(Y).unsqueeze(-1)  # (N,1)\n",
    "loader = DataLoader(TensorDataset(X,Y), batch_size=16, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SeqModel(nn.Module):\n",
    "    def __init__(self, rnn_type='RNN', input_size=1, hidden=16):\n",
    "        super().__init__()\n",
    "        if rnn_type=='RNN':\n",
    "            self.rnn = nn.RNN(input_size, hidden, batch_first=True)\n",
    "        elif rnn_type=='LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, h = self.rnn(x)\n",
    "        if isinstance(h, tuple):\n",
    "            h = h[0]\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "models = {t: SeqModel(t).to(device) for t in ('RNN','LSTM','GRU')}\n",
    "opt = {t: torch.optim.Adam(m.parameters(), lr=0.01) for t,m in models.items()}\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "# one epoch quick run\n",
    "for t,m in models.items():\n",
    "    m.train()\n",
    "    total_loss=0.0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        opt[t].zero_grad()\n",
    "        out = m(xb)\n",
    "        loss = crit(out, yb)\n",
    "        loss.backward()\n",
    "        opt[t].step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"{t} — avg loss: {total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649328b",
   "metadata": {},
   "source": [
    "[Pytorch Transformers from Scratch (Attention is all you need)](https://www.youtube.com/watch?v=U0s0f995w14)\n",
    "\n",
    "*Videodaki projeyi yapınız*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers from Scratch — Multi-Head Self-Attention minimal implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([module for _ in range(N)])\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=32, num_heads=4):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.d_k = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, x):\n",
    "        B, T, E = x.shape\n",
    "        qkv = self.qkv(x)  # (B,T,3E)\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "        # reshape for heads\n",
    "        q = q.view(B, T, self.num_heads, self.d_k).transpose(1,2)  # (B,heads,T,d_k)\n",
    "        k = k.view(B, T, self.num_heads, self.d_k).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_heads, self.d_k).transpose(1,2)\n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) / (self.d_k ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, v)  # (B,heads,T,d_k)\n",
    "        out = out.transpose(1,2).contiguous().view(B,T,E)\n",
    "        return self.out(out)\n",
    "\n",
    "# Quick test\n",
    "x = torch.rand(2,10,32)\n",
    "attn = MultiHeadSelfAttention(32,4)\n",
    "out = attn(x)\n",
    "print('Input shape:', x.shape, 'Output shape:', out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3277a",
   "metadata": {},
   "source": [
    "[Implementing the Self-Attention Mechanism from Scratch in PyTorch!](https://www.youtube.com/watch?v=ZPLym9rJtM8)\n",
    "\n",
    "*Videodaki projeyi yapınız*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention (single-head) — compact implementation and check\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v, mask=None):\n",
    "    # q,k,v: (B, T, D)\n",
    "    scores = torch.matmul(q, k.transpose(-2,-1)) / (q.size(-1)**0.5)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(attn, v), attn\n",
    "\n",
    "B,T,D = 1,5,8\n",
    "q = torch.rand(B,T,D)\n",
    "k = torch.rand(B,T,D)\n",
    "v = torch.rand(B,T,D)\n",
    "out, attn = scaled_dot_product_attention(q,k,v)\n",
    "print('out.shape', out.shape, 'attn.shape', attn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6575a95e",
   "metadata": {},
   "source": [
    "[Learn PyTorch in 5 Projects – Tutorial](https://www.youtube.com/watch?v=E0bwEAWmVEM&t=12974s)\n",
    "\n",
    "*Videodaki son projeyi yapınız (Transformer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25377195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal Transformer Encoder Block — tiny demo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=32, heads=4, ff_dim=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "# quick forward pass\n",
    "x = torch.rand(2,10,32)\n",
    "block = TransformerEncoderBlock(32, heads=4)\n",
    "out = block(x)\n",
    "print('Transformer block input', x.shape, 'output', out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7f3af",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aee247",
   "metadata": {},
   "source": [
    "*Tebrikler ✨*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
